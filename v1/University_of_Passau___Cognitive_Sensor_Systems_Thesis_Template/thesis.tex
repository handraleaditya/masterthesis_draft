% Thesis Template for MSc, BSc Theses at University of Passau for the Professorship for Cognitive Sensor Systems 
%   Written by by Alexander Gall <alexander.gall@uni-passau.de>, October 2025
%   https://www.fim.uni-passau.de/kognitive-sensorsysteme

% Acknowledgements:
%   Adapted from ITW Thesis Template - Chair Prof. Krämer


\documentclass{thesisclass}

% NOTE: Use your preferred language.
 \usepackage[english]{babel}%<--- TODO: If you want to write your thesis in English don't uncomment this command by putting a % sign before the command.
% \usepackage[ngerman]{babel}%<--- TODO: If you want to write your thesis in German, uncomment this command by removing the % sign before the command.


\usepackage{graphicx} 											
\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\graphicspath{{./figures/}} %TODO: Save your figures in the figures folder, if you want to include graphics in your thesis. This template will reference to that folder.									
%\usepackage[natbibapa]{apacite}
\usepackage{cite} 
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsthm}
\usepackage[]{todonotes}
\usepackage{etoolbox}
\makeatletter
\patchcmd{\chapter}{\if@openright\cleardoublepage\else\clearpage\fi}{}{}{}
\makeatother
\usepackage{chngcntr}
\counterwithout{footnote}{chapter}
\counterwithout{equation}{chapter}
\counterwithout{table}{chapter}
\counterwithout{figure}{chapter}
\makeatletter
\patchcmd{\scr@startchapter}{\if@openright\cleardoublepage\else\clearpage\fi}{}{}{}
\makeatother
\usepackage{mathptmx}
\setkomafont{disposition}{\bfseries}
\usepackage{pifont}
\usepackage{adjustbox}
\usepackage{caption}
\usepackage{longtable}
\usepackage{float}
 

% TODO: Uncomment just one of the following commands, according to your thesis type. Seminar Thesis is set by default.

%\newcommand{\mytype}{\iflanguage{english}{Bachelor's Thesis}{Bachelorarbeit}} 
\newcommand{\mytype}{\iflanguage{english}{Master's Thesis}{Masterarbeit}} 
%\newcommand{\mytype}{\iflanguage{english}{Seminar Thesis}{Seminararbeit}} 

% TODO: Enter personal information and information about your thesis.
\newcommand{\myname}{\iflanguage{english}{Aditya Handrale} {Vorname Nachname}}
\newcommand{\matricle}{108489}
\newcommand{\mytitle}{\iflanguage{english}{DatasetWiz: A Visual Analytics Framework for the Comparative Analysis of Dimensionality Reduction and Clustering Quality} {Titel der Arbeit}}
\newcommand{\mycurriculum}{\iflanguage{english}{curriculum} {Studiengang}}
\newcommand{\myinstitute}{\iflanguage{english}
{Chair of Cognitive Sensor Systems}
{Professur für Kognitive Sensorsysteme}}

% TODO:change the names of your reviewer and advisor to your actual needs.
\newcommand{\reviewerone}{Prof. Dr. Christoph Heinzl \orcid{0000-0002-3173-8871}}
\newcommand{\reviewertwo}{Prof. \orcid{0000-0002-3173-8871}}
\newcommand{\advisor}{Anja Heim\orcid{0000-0002-3670-5403}}


\newcommand{\timeend}{\iflanguage{english}{Passau, DD.MM.20YY}{Passau, DD. Monat.20YY}} % TODO: Insert submission date.
\newcommand{\submissiontime}{DD.MM.20YY} % TODO: Insert submission date which will be displayed in your Declaration.

\hypersetup{
 pdfauthor={\myname},
 pdftitle={\mytitle},
 pdfsubject={\mytype},
 pdfkeywords={\mytype}
}

% Make \subsubsection look like \subsection
\makeatletter
\renewcommand\subsubsection{%
  \@startsection{subsubsection}{3}{\z@}%
    {-3.25ex\@plus -1ex \@minus -.2ex}%
    {1.5ex \@plus .2ex}%
    {\normalfont\large}%
}
\makeatother



%-----------------CUSTOM PREAMBLE----------------------

% FFro tables
\usepackage{booktabs}
\usepackage{tabularx}

%for code snippets
\usepackage{listings}
\usepackage{xcolor}

% Define custom colors for code highlighting
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
% A slightly lighter and cleaner background color
\definecolor{backcolour}{rgb}{0.98,0.98,0.98}

% Setup the style for JavaScript (same as before)
\lstdefinelanguage{JavaScript}{
  keywords={break, case, catch, continue, debugger, default, delete, do, else, false, finally, for, function, if, in, instanceof, new, null, return, switch, this, throw, true, try, typeof, var, void, while, with, const, let},
  morecomment=[l]{//},
  morecomment=[s]{/*}{*/},
  morestring=[b]',
  morestring=[b]"
}

% Improved general style for listing
\lstset{
    language=JavaScript,
    backgroundcolor=\color{backcolour},
    commentstyle=\itshape\color{codegreen}, % Italic comments
    keywordstyle=\color{magenta}\bfseries, % Bold keywords
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize, % Use a good monospaced font
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=none, % **** LINE NUMBERS REMOVED HERE ****
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    frame=lines, % **** Only top and bottom lines ****
    framerule=1pt, % Slightly thicker frame lines
    rulecolor=\color{codegray}, % Subtle gray color for the frame
    abovecaptionskip=1.5em, % More space above the caption
    xleftmargin=1.5em, % Indent the entire block slightly
    xrightmargin=1.5em % Indent the entire block slightly
}

\begin{document}

%Do not change anything until the next TODO appears, unless the title is so logn that the title page spreads over two pages. In this case, reduce the vertical spaces highlighted by the blue comments below 

\pagenumbering{arabic}
\newcommand{\diameter}{20}
\newcommand{\xone}{-15}
\newcommand{\xtwo}{160}
\newcommand{\yone}{15}
\newcommand{\ytwo}{-253}

\begin{titlepage}
	
	\begin{center}
    \center
		\iflanguage{english}
		{\includegraphics[width=.5\textwidth]{logos/uni_1200dpi_fb_gross_EN.png}}
		{\includegraphics[width=.5\textwidth]{logos/uni_1200dpi_fb_gross.png}}		
	\end{center}
	\begin{textblock}{10}[0,0](10,2.5)
	\end{textblock}
	\changefont{ppl}{m}{n}
	\vspace*{0.5cm}
	\begin{center}
		\huge{\mytitle}
		\vspace*{0.35cm}\\ % reduce vspace if title page spreads over two pages
		\Large{
			\iflanguage{english}{\mytype\\of}			
			{\mytype\\von}
		}\\
		\vspace*{0.35cm}
		\huge{\myname}\\
		\Large{\matricle}\\
        \vspace*{0.35cm} % reduce vspace if title page spreads over two pages
        \Large{
			\iflanguage{english}{as part of the study program}			
			{im Rahmen des Studiums}
		}\\
        \vspace*{1cm}
        \Large{
			\iflanguage{english}{\mycurriculum \\ at the}			
			{\mycurriculum \\ an der}
		}\\
		\vspace*{0.35cm} % reduce vspace if title page spreads over two pages
		\Large{
			\iflanguage{english}{University of Passau \\ Faculty of Computer Science and Mathematics}			
			{Universität Passau \\ Fakultät für Informatik und Mathematik}
			\vspace*{0.5cm}\\
			\myinstitute
		}
	\end{center}
	\vspace*{0.15cm} % reduce vspace if title page spreads over two pages
	\Large{
		\begin{center}
			\begin{tabular}[ht]{l c l}

				\iflanguage{english}{Advisor}{Betreuer}: & \hfill  & \reviewerone\\
 				\iflanguage{english}{Assistance}{Betreuender Assistent}: & \hfill  & \advisor\\
			
			\end{tabular}
		\end{center}
	}
	
	
	\vspace{0.1cm} % reduce vspace if title page spreads over two pages
	\begin{center}
		\timeend
	 \end{center}
	
	
\end{titlepage}

% Add a table of contents
\tableofcontents
\newpage


\clearpage

\chapter*{Acknowledgments}
% TODO: Add your acknowledgements or comment this chapter to hide it.
First and foremost, I would like to express my sincere gratitude to my supervisor, Prof. Dr. Christoph Heinzl, for his invaluable guidance, expertise in visualization research, and unwavering support throughout this thesis journey. His insights into cognitive sensor systems and visual analytics have been instrumental in shaping this work.

I extend my heartfelt appreciation to Anja Heim, my academic assistant, whose patient guidance, constructive feedback, and technical expertise helped me navigate the complexities of visualization design and implementation. Her dedication to helping students succeed is truly commendable.

My deepest gratitude goes to my industry partners at Robert Bosch GmbH: Johannes Mohren and Dr. Sabrina Schmedding. Their real-world perspective on industrial quality assessment challenges provided the essential context that transformed this from a purely academic exercise into a meaningful contribution to industrial practice. The datasets, domain knowledge, and collaborative discussions were invaluable to this research.

I am grateful to the University of Passau for providing the academic environment and resources necessary for this research, and for fostering interdisciplinary collaboration between academia and industry.

Special thanks to my family, my parents and brother, who provided unwavering emotional support and encouragement throughout the challenging periods of this thesis. Their constant reminders to stay focused and not procrastinate (which I may have occasionally ignored) kept me on track during the most demanding phases of this work.


\pagebreak

\chapter*{Abstract}
% TODO: Enter your abstract here with a maximum of 200 words.
\noindent In data intensive domains such as manufacturing and medical diagnostics, the success of predictive models is primarily driven by the quality and separability of high-dimensional feature spaces. For practitioners to analyze various properties of these datasets, feature extraction algorithms generate hundreds of features, which are further analyzed using dimensionality reduction (DR) and clustering techniques. An accurate understanding of these high dimensional feature spaces is crucial, especially since data scientists and machine learning engineers make downstream decisions based on the understanding of the feature space. 
A comparison of various dimensionality reduction techniques, both quantitatively using metrics and qualitatively using visualization is absolutely vital for investigation of the dataset "trainability” and feature space quality assessment.  
As of now, practitioners rely on "black box” projections and static score tables when analyzing the quality of the high dimensional feature spaces. Visual inspection of lower dimensional projections of these feature spaces is often used to investigate the quality of the dataset and to find various effects, artifacts and outliers. The quality metrics and dimensionality reduction methods must be compared manually, which makes this task time consuming, error prone and cognitively demanding. This thesis aims to support the domain experts in the evaluation of the dataset suitability for downstream classification tasks.  

To tackle these challenges, our work introduces DatasetWiz, a comparative visual analytics framework that provides a comprehensive understanding of the feature space quality and projection reliability using summary visualization and two novel visualization techniques. Various dimensionality reduction methods are used to summarize the high dimensional structures and are rendered in a side by side comparison tool called DimCompare (Synchronized dual view scatterplots). Information about why the clusters form is calculated statistically and then visualized using Feature Contribution Glyphs (Cluster annotations that highlight feature level differences). The aggregate performance of the different techniques can be explored in a composite visualisation called BarDar Chart (Summary overview combining Bar chart and Radar chart). The efficacy and usefulness of these visualisations are demonstrated using case studies and a user study with X participants. [The results indicate that DatasetWiz successfully facilitates the identification of structural patterns and artifacts, thereby improving the efficiency of early-stage data diagnostics.]


\pagebreak

% TODO: Uncomment the lines to add your abstract in german.
%\chapter*{Zusammenfassung}
    % TODO: Enter your abstract here with a maximum of 200 words.
%\pagebreak


\setcounter{page}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ACTUAL THESIS BEGINS HERE %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}

In the past few years, high-dimensional data has grown more prevalent in fields like manufacturing, medical diagnostics, environmental monitoring, and quality control. As machine learning has gained popularity across multiple domains, feature extraction techniques, be they statistical, domain-specific, or neural network-oriented, now generate hundreds or even thousands of dimensions/features for a singular data item. These feature vectors often include a lot of information in them, but since they are so complex, it's hard for people to understand, compare, or think about these high-dimensional representations. To address the complexity of these high-dimensional spaces, practitioners make use of Dimensionality Reduction (DR), which is the process of mapping high dimensional data into a lower dimensional representation (typically 2D or 3D), while attempting to preserve the original structural relationships.
\section{Motivation and problem statement}
  

This problem is especially important in factories and industries, where datasets might not be balanced, might have a lot of noise, or might have been collected under less than optimal settings. Before spending time and money on training a model, machine learning programmers and domain specialists need to perform a trainability assessment to verify whether a dataset is "learnable”or not. This means that the data has enough structure, class separation, or structural signal on its own to make modeling useful. Unfortunately, the technologies we have presently don't assist us in trainability assessment very much. Black-box dimensionality reduction projections, static 2D visualizations like scatterplots, and clustering score tables only show part of the picture. This means that users sometimes have to trust their gut feelings or judgment. Conventional visualization methods do not facilitate the interactive exploration and comparative analysis that is crucial for successful decision-making in industrial settings. 

The need for this thesis arose from the disparity between high-dimensional feature representations and human comprehension. A prior research collaboration with Bosch underscored the importance for enhanced feature space diagnostics. The partnership's main goal was to find flaws in industrial hardware parts using images, but the basic problem, i.e. understanding high-dimensional embeddings and their structure, goes much beyond merely images. In fact, the tools that were built in this thesis were tested on conventional, high dimensional CSV-based datasets, such as those that assess air pollution or material strength. This shows that the tools can handle a wide range of data.

\begin{figure}[H] %NOTE: This command creates a figure environment. An environment always starts with a \begin{} command and ends with an \end{} command. 
	\begin{center} %<-- this command aligns your figure as desired, here the figure will be centered.
		\includegraphics[width = 0.9\textwidth]{images/pipeline.jpg} %this includes you graphic stored in your figures folder. Upload works via the upload button on the left side.
		\caption{Feature-space analysis pipeline combining dimensionality reduction, visualization, quantitative metrics, and downstream machine learning}%this command creates a caption.
		\label{fig:IA_Envrionment}% this is the aforementioned label, which can be now referred by name. Here it would be \ref{fig:valuchain}. 
	\end{center}
\end{figure} 

More broadly speaking, analyzing and comparing high-dimensional feature spaces is difficult for several reasons: 
  
\begin{enumerate}
  \item \textbf{Projection Instability}: Different Dimensionality Reduction (DR) techniques (PCA, t-SNE, MDS, UMAP) produce inconsistent feature visualizations, each emphasizing different structures. No single method provides a complete picture.
  \item \textbf{Information loss}: Reducing dimensions often distorts some feature relationships. Clusters may appear well-separated in 2D but actually overlap in the original space, or vice versa.
  \item \textbf{Lack of visual and quantitative integration}: Clustering metrics like Silhouette Score offer objective numeric values, but interpreting them in context is difficult without visual feedback.
\end{enumerate}  

This thesis aims to fill the gap between complex high-dimensional feature spaces and human understanding by introducing novel interactive visualization tools. These tools aim to combine dimensionality reduction, clustering quality metrics, and feature-based visual encodings to support early decision-making in the data science pipeline, for tasks such as evaluating dataset, pointing out anomalies, or identifying the requirement for further preprocessing. Rather than simply illustrating DR results, the goal here is to help the end users, engineers and data scientists, interpret, compare, and assess the quality and separability of high-dimensional data in an intuitive way.





\section{Research questions}

This research is guided by five connected research questions that aim to address both the theoretical and the practical aspects of high-dimensional data visualization in industrial contexts:

\textbf{Research Question 1 (RQ1):} Feature Relationship Understanding  
\begin{itemize}
  \item How can interactive visualization reveal which features drive cluster formation across different dimensionality reduction techniques?   
\end{itemize}
This question seeks to resolve the interpretability challenge that is often posed in high-dimensional feature spaces, where understanding the feature contributions are essential for quality assessment and evaluation, but difficult to achieve with traditional visualization approaches.

\textbf{Research Question 2 (RQ2):} Comparative Evaluation of Dimensionality Reduction  
\begin{itemize}
  \item How can we effectively enable side-by-side comparison of different dimensionality reduction results to support algorithm selection and validation?  
\end{itemize}

This question focuses on the practical needs for practitioners to understand how different dimensionality reduction method choices affect representation of the data and its subsequent analysis.

\textbf{Research Question 3 (RQ3):} Clustering Quality Evaluation and Visualization
\begin{itemize}
  \item How can composite integrated visualisation effectively facilitate comparative assessment of dimensionality reduction techniques through the aggregation of conflicting quality metrics?
\end{itemize}

This question addresses the problem of interpreting multiple, and potentially conflicting, quality measures in a visual framework.

\textbf{Research Question 4 (RQ4):} Pattern Identification Across DR Techniques 
\begin{itemize}
  \item How can coordinated multiple views and interactive exploration support the validation of structural patterns and identification of projection induced artifacts across different DR methods?
\end{itemize}
This question explores the robustness of patterns that are discovered and also the identification of DR technique-specific artifacts.



\section{Contributions}
This thesis makes several novel contributions in the domains of data/information visualization, industrial quality assessment and visual analytics.
\subsection{Interactive Visualization Techniques}
\textbf{DimCompare Dual-View system:} We introduce a novel, dual-view approach for comparing two different dimensionality reduction techniques via  synchronized, interactive scatterplots. Compared to traditional single view 2D scatterplots, DimCompare allows for real-time linked interaction between different dimensionality reduction techniques and representations. DimCompare also enables brushing, selection and coordinated exploration.

\textbf{Dynamic feature contribution glyphs} : We developed an innovative cluster annotation "glyphs” that visualize which features contribute most to a cluster formation, dynamically. These cluster annotations provide immediate visual feedback about the feature importance differences between the clusters, which update in real time, as users  explore different data subsets.

\textbf{Bardar Composite Visualization:} We created a composite integrated visualiation that utilizes explicit encoding (bar charts) to overcome the well documented perceptual limitations of radar charts (area bias), hence providing an objective ranking of DR method reliability. The integrated design provides users with both detailed metric visualization and also aggregate the performance ranking, enabling more effective metric comparisons across multiple dimensionality reduction techniques.



\subsection{Technical implementation}

\textbf{Web-based architecture:}  Development of a complete web-based visualization framework, using Django backend, D3.js frontend, which enables browser-based internet access to visualization capabilities without requiring specialized installation of software.

\textbf{Scalable Data pipeline:} Implementation of efficient algorithms for dimensionality reduction, clustering and metric calculations, that can handle large, industrial scale CSV datasets, while still maintaining, near-real time, interactive performance.

\textbf{Open source framework:} Creation of an open source implementation, that is extensible, and enables reproducible research, while also providing a foundation for future visualization tool development.

 


\section{Chapter summary} 
This chapter introduced the motivation, scope and contributions of this thesis. It establishes the core challenge of assessing the “trainability” of a dataset, or separation of high dimensional feature spaces. This is a critical problem in the Industrial AI and quality assessment, as highlighted by the collaboration with Bosch. The key points discussed are : 

\begin{itemize}
  \item \textbf{Core problem:} Identified the black box/uninterpretable nature of feature spaces generated as the main problem. The success of a ML model, further in the pipeline, is dependent on this data, hence leads to uncertainty in the data science pipeline.
\item \textbf{Identified gaps:} We defined some specific limitations of the current methods, like projection instability (different algorithm = different results), information loss (2D projections lose information) and the disconnect between quantitative metrics and qualitative visual inspection.
\item \textbf{Research questions:} Synthesised the core research questions (RQs) that guide this work, focusing on the feature level understanding, DR comparison, Metric visualization, pattern identification and industrial validation.
\item \textbf{Novel Contributions:} Introduced the two primary contributions of this thesis, the DimCompare system for visual comparison exploration and the BarDar chart for quantitative metric comparison.
\end{itemize}










%----------------------END OF CHAPTER 1----------------------

\newpage
\chapter{Background and related work}
This chapter lays the foundation for this thesis by reviewing some basic ideas and existing research that is important for analyzing high-dimensional data, machine learning, and visual analytics. It covers key areas like extracting features from images, methods for dimensionality reduction, clustering algorithms and their related quality metrics, and different methods to visualize and understand High dimensional data.


\section{Understanding High dimensional data}
High-dimensional data refers to the datasets where each of the sample or row is described by a large number of features or variables, often dozens, hundreds, or even thousands. These features can come from a wide variety of sources, including sensor readings, material strength measurements, air quality parameters, or neural network embeddings for images. In this thesis, high-dimensional data is primarily handled in the form of structured CSV files, where each row represents a data instance/sample and each column corresponds to a feature, usually numeric. Such high dimensional data is common in real-world domains like environmental monitoring, quality control, and manufacturing process analysis. However, as the number of dimensions/features increases, it becomes increasingly challenging to explore, visualize or extract meaningful patterns from the data using traditional visualization techniques. This is commonly known as the curse of dimensionality, and it motivates for the use of dimensionality reduction techniques to uncover structure and gives support in interpretation.


\subsection{Curse of dimensionality}
Analyzing high dimensional data has been recognized as one of the fundamental problems in machine learning and data analysis \cite{zebari2020comprehensive}. As also noted by Jia et. al. \cite{jia2022feature}, the curse of dimensionality significantly increases computational costs and the storage requirements, while also negatively impacting the accuracy and efficiency of the data analysis methods/algorithms. There is an exponential increase in data sparsity and computational demands as dimensionality grows \cite{bellman1957dynamic} \cite{peng2023interpreting}.

This phenomenon is especially evident in image applications where CNN(Convolutional Neural Network) based feature extractors (VGG-19, ResNet-50 etc) are used to generate thousands of features from a single image, creating complex feature spaces that are difficult to interpret as well as computationally intensive. 

Anowar et. al. \cite{ANOWAR2021} gives a complete comparison of dimensionality reduction algorithms. They categorize them into primarily linear vs non linear  and supervised vs unsupervised approaches. The empirical analysis performed by them across challenging datasets clearly demonstrates that different dimensionality reduction techniques excel in different contexts, underscoring a need for a comparative analysis tools that can help practitioners select appropriate methods for their specific applications



\subsection {Image feature extraction in industrial manufacturing }
To evaluate properties like structural integrity of industrial hardware, high dimensional representations are generated by using deep neural network based image feature extractors. In current manufacturing workflows, pretrained image feature extraction models such as VGG-19 (Visual Geometry Group) and ResNet-50 (Residual Network) are used to convert visual inspection data into numerical feature vectors, consisting of thousands of dimensions. This work also briefly focuses on analyzing the resulting feature spaces to identify production line induced characteristics such as defects and manufacturing inconsistencies.

\section{Dimensionality Reduction Techniques}
Dimensionality reduction is a key set of methods that are used to change the data from a higher dimensional space into a lower dimensional one, while also trying to keep the important properties and structure present in the original data. \cite{sorzano2014survey}. These dimensionality reduction methods are generally split into linear and non linear data, where each has different features and compromises. The fact that there exists a vase selection of dimensionality reduction techniques, each having its own biases and compromises, directly indicates why comparative visualization systems are needed. Building such a comparative visualization system is the main goal of this thesis.

\subsection{Principal Component Analysis (PCA) }  
This is the most widely used and commonly known Dimensionality reduction method due to its mathematical simplicity and interpretability \cite{SalihHasanAbdulazeez2021}. It projects the data in a lower dimensional space. PCA does this by finding orthogonal components (principal components) that capture the maximum variation in the data. The linear nature of the technique provides a significant advantage for preserving global structures and relationships in the data \cite{jolliffe2016pca}. 


Boileau et al. \cite{boileau2020scPCA} extend traditional PCA through sparse contrastive PCA, which works by extracting sparse, stable and interpretable features by leveraging control data. Their work demonstrates how PCA variants can be enhanced to address specific domain requirements , particularly in biological applications where interpretability is crucial.

PCA is fast to compute, predictable, and good at keeping the overall structure of the data \cite{jolliffe2016pca}. However, the downsides of PCA are that it might not capture complex, non-linear relationships present in the data. Which has led to development of  kernel PCA and other non-linear extensions \cite{scholkopf1998nonlinear}. The technique works best when the underlying data is somewhat linear, but it may also miss important patterns in the data that have complex non-linear relationships, such as those found commonly in industrial image analysis applications.



\subsection{t-Distributed Stochastic Neighbor Embedding (t-SNE) }  
This is a powerful non-linear method of dimensionality reduction that is really good at showing the local structures present in the data, often revealing tight groups of data points that are similar. t-SNE was introduced by \cite{vandermaaten2008tsne}, and it revolutionized the visualization of high dimensional data, by preserving  local neighborhood structures, while reducing dimensionality.

t-SNE  looks at similarities as probabilities and tries to match these probabilities in both the high and low dimensional spaces. It models similarities between data points using probability distributions and minimizes the Kullback-Leibler divergence (KL) between high-dimensional and low-dimensional representations.


It is great at showing clusters that might be hidden in high dimensional space and at preserving local relationships. Making it really valuable for exploratory data analysis (EDA) and pattern discovery. However t-SNE has many limitations that affect the interpretation, it sometimes distorts global structure \cite{kobak2019art} and the results obtained are sensitive to hyperparameter settings (like perplexity) \cite{wattenberg2016how}. Since t-SNE is stochastic in nature, it produces slightly different results in different runs \cite{vandermaaten2013barnes}. t-SNE is also computationally expensive \(\mathcal{O}(nˆ2)\) and can sometimes twist the overall structure, versions like Barnes-hut t-SNE help mitigate this \cite{vandermaaten2013barnes}.

The non-deterministic nature of this method makes it challenging to reproduce results, increasing the importance of setting random seeds and understanding the impact of hyperparameters upon the final visualisations.


\subsection{Multidimensional Scaling (MDS) }   
Multidimensional Scaling (MDS) is a classical dimensionality reduction technique that aims at preserving pairwise distances between the different data points, when projecting high dimensional data into lower dimensional space. Originally developed in the field of psychometrics, it is now widely used across various domains. MDS works by transforming a dissimilarity matrix into a geometric configuration in fewer dimensions, while still maintaining the relative spatial relationships of the data. \cite{Borg2005MDS}  

Unlike  linear and variance based methods like PCA, MDS is distance-preserving. It attempts to place each data point in a low dimensional space such that the -inter-object distances are preserved as faithfully as possible. MDS works best when input distances are euclidean, and the data conforms to metric assumptions \cite{KruskalWish1978}. There also exist variants of MDS such as non-metric MDS, that relax these assumptions by only preserving the rank of order distances, which makes it more robust to non-linear data structures.

This dimensionality reduction method tried to keep the distances between the data points as much as possible in the lower dimensional space. MDS is  good for understanding how far apart items are from each other, but it takes a lot of computing power for larger datasets, (with a computational time complexity of \(\mathcal{O}(n^3)\) for exact calculation, where N refers to the number of data points). This makes it less practical for datasets without approximation strategies \cite{Venna2010}. Because it needs so much computing, it's usually preferred for smaller datasets. Moreover, it does not explicitly model local or global structure tradeoffs the way t-SNE or UMAP do. Therefore MDS can struggle to highlight cluster boundaries and maintain neighborhood relationships in sparse datasets. MDS provides a valuable contrast to techniques like PCA, t-SNE and UMAP. Its role in this thesis is to primarily serve as a comparative benchmark.

 
\subsection{Uniform Manifold Approximation (UMAP) }
UMAP was developed by \cite{McInnes2018UMAP}, and it addressed several limitations of t-SNE while maintaining the ability to preserve local structure. It is based on manifold learning theory and topological data analysis, and provides faster computation than t-SNE while better preserving global structure alongside local neighborhoods.

This method constructs a high dimensional graph representation of the data and then optimises low dimensional graphs to be as structurally similar to the high dimensional graph as possible. This kind of approach allows UMAP to handle larger datasets more efficiently than t-SNE, while also producing more stable results across multiple different runs.

UMAP has the ability to preserve both local and global structures, which makes it particularly valuable for industrial applications where understanding both the detailed cluster structure and overall data structure is important, like exploratory analyses in industrial datasets \cite{Ghojogh2021UMAPSurvey}. However, the technique introduces its own set of hyperparameters and assumptions that can impact the final visualization, re- emphasizing the need for competitive analysis tools.
 

\subsection{Other techniques : Isomap and Autoencoders}
Beyond PCA, t-SNE, UMAP and MDS, there exist additional dimensionality reduction techniques which offer alternative perspectives on high dimensional data, like Isomap and Autoencoders.
Isomap \cite{Tenenbaum2000Isomap} extends the classic MDS technique by incorporating geodesic distances, instead of euclidean ones, which allows it to preserve the intrinsic geometry of non-linear manifolds. It constructs a neighborhood graph, and computes the shortest path distances between the points. However, Isomap is sensitive to noise and outliers, and its performance degrades if the neighborhood graph is poorly constructed. 

Autoencoders, on the other hand, are unsupervised neural network models that learn to compress the data into lower-dimensional latent representations and reconstruct it back to the original space. \cite{Hinton2006Autoencoders}. This learned embedding captures the non-linear dependencies and is highly flexible due to the representational power of deep neural networks. There also exist variants such as denoising autoencoders or Variational Autoencoders (VAEs) that have further improved robustness and generative capabilities. However, autoencoders typically require very large training data sets and careful tuning, to learn trivial representations and avoid overfitting.


While these techniques were not the focus of the implementation in this thesis, they offer valuable alternatives and are potential candidates for the future extensions of comparative visualization tools, particularly in deep learning focused workflows.


\begin{table}[h]
	\centering
	\begin{tabular}{lccc}
	\hline
	\textbf{Method} & \textbf{Linearity} & \textbf{Structure Preservation} & \textbf{Time Complexity}\\
	\hline
	PCA & Linear & Global (variance) & $\mathcal{O}(nd^2)$ \\
	t-SNE & Nonlinear & Local & $\mathcal{O}(n^2)$ \\
	UMAP & Nonlinear & Local + Global & $\mathcal{O}(n \log n)$ \\
	MDS & Nonlinear & Distance preserving & $\mathcal{O}(n^3)$ \\
	\hline
	\end{tabular}
	\caption{Summary of Dimensionality Reduction Techniques}
	\end{table}
	



\section{Comparative analysis challenges}
Espadoto et. al \cite{Espadoto} provides a quantitative survey of various dimensionality reduction techniques, it evaluates how these DR methods perform across a wide variety of datasets and metrics. The study shows that no single method always outperforms the others, and each one ever gives only a partial or biased view of the high dimensional data, which underscores the need for comparative analysis tools.  This is why the core design of DimCompare is justified. 

The authors identified several challenges in evaluating dimensionality reduction techniques: 
\begin{enumerate}
  \item Lack of ground truth, which makes it difficult to assess the quality of the projections.
  \item Tradeoffs exist between local and global structure preservation.
  \item Dataset characteristics, such as noise and dimensionality, have an influence on the dimensionality reduction performance.
  \item Computational scalability becomes important for practical use on high-dimensional datasets.
\end{enumerate}
 
These challenges emphasize the need for visualization tools, like the ones developed in this thesis. These tools help practitioners navigate the trade-offs and make informed choices based on context-specific requirements.
 
%-----checkpoitn charli
 
\section{Clustering algorithms for feature space analysis}
Clustering algorithms help us identify groups of similar data points that are present in the feature space and to detect outliers, which is essential for evaluating the structure and separability of high dimensional datasets. Clustering methods are commonly classified into four categories, 
Partitioning methods (e.g., k-Means, GMM), which assign points into clusters, based on minimizing the within-cluster variance or maximizing likelihood.
Density-based methods (e.g., DBSCAN, HDBSCAN), which find clusters by identifying dense regions separated by sparse areas.
Graph-based methods (e.g., Spectral clustering), which make use of eigenvectors of similarity matrices to reveal structure.
Subspace/high-dimensional methods (e.g., SUBCLU, ENClust) which discover clusters that exist in subsets of dimensions.


\subsection{k-Means Clustering}
k-Means is a centroid-based algorithm that partitions data into k clusters  by minimizing within-cluster variance using an iterative refinement method \cite{lloyd1982}. k denotes the number of clusters, a hyperparameter chosen either based on prior knowledge or quality metrics such as the silhouette score.
\[
\min_{C} \sum_{i=1}^{k} \sum_{x \in C_i} \| x - \mu_i \|^2
\]


\begin{itemize}

  
  \item \(n\) = number of data points,

  \item \(k\) = number of clusters,
  \item \(d\) = dimensionality,
  \item \(I\) = number of iterations.
    \item Time complexity: Typically \(
	\mathcal{O}(I \cdot n \cdot k \cdot d)
	\) 
 
\end{itemize}

It is efficient, easy to implement and compatible with many internal cluster metrics. However, the drawbacks of k-Means are that it assumes spherical clusters of similar size, and is sensitive to outliers and initialization while also requiring to specify \(k\). Because k-Means assumes spherical clusters of similar size, visual inspection with DimCompare can help assess whether this assumption holds in practice.

\subsection{DBSCAN (Density-Based Spatial Clustering of Applications with Noise)}
A density-based method of clustering that identifies arbitrary shaped clusters based on density connectivity and marks low-density points as noise \cite{ester1996dbscan}.
DBSCAN defines clusters as areas of high density separated by sparse regions. A point \(p\) is considered a core point if at least \(minPts \) neighbors fall within a radius \(\varepsilon\). Clusters are formed by density connectivity.

\[
|\{q : \|q - p\|\leq \varepsilon\}| \ \ge \ \text{minPts}
\]
\begin{itemize}
	\item \(\varepsilon\) = neighborhood radius,
	
	\item \(minPts \) = minimum number of neighbors.
	\item Time complexity: Typically \(
		\mathcal{O}( n \log n )
		\) 
\end{itemize}
 

It works well for non-convex clusters and clusters of arbitrary shapes and also detects noise. But choosing appropriate distance thresholds can be challenging, especially in high dimensional spaces where distance metrics start becoming less and less meaningful \cite{Schubert2017DBSCANRevisited}.

\subsection{HDBSCAN (Hierearchical Density-Based Spatial Clustering of Applications with Noise)}
An extension of DBSCAN, HDBSCAN constructs a hierarchical clustering structure and extracts clusters based on their stability, and extracts the stable ones\cite{Campello2013HDBSCAN}. 

It eliminates the need to choose epsilon and automatically determines the number of clusters.
Time complexity : 
\(
\mathcal{O}(n^2)
\)

It is good at discovering clusters of varying densities and identifying outliers without specifying \(k\). This makes it suitable for practical exploratory data analysis scenarios where cluster counts are often unknown or feature spaces contain a lot of noise. Its weaknesses are  that it may over split clusters in noisy high dimensional spaces and it is more computationally intensive.



\subsection{Spectral Clustering}
Spectral clustering is a graph based technique of clustering that uses eigenvectors of the similarity matrix's Laplacian to cluster data in reduced dimensions \cite{vonLuxburg2007}. It is particularly effective at capturing complex cluster structures, but it scales poorly with large scale datasets, due to the costs related to eigen decomposition.

Spectral clustering uses the eigenvectors of a graph Laplacian L=D minus W derived from a similarity matrix W, to embed data before clustering.
\[
L = D - W
\]

where \(W\) is the similarity (adjacency) matrix and \(D\) is the degree matrix with \(D_{ii} = \sum_{j} W_{ij}\).

Parameters: Similarity function (e.g., Gaussian kernel), number of clusters \(k\)  
Time Complexity:  \(\mathcal{O}(n^3)\)


\subsection{Gaussian Mixture Models (GMM)}
This is a probabilistic, soft clustering approach that assumes that data is generated from a mixture of various Gaussian distributions. GMM's provide probabilistic class assignments and can model covariance structures well \cite{McLachlanPeel2000GMM}. However, they often do not perform well when clusters deviate significantly from Gaussian shapes and require very careful hyperparameter tuning.

\[
p(x \mid \theta) = \sum_{i=1}^{k} \pi_i \, \mathcal{N}(x \mid \mu_i, \Sigma_i)
\]



 


\subsection{Subspace clustering}
These types of methods, like SUBCLU, identify clusters present in specific subsets of dimensions \cite{Kailing2004SUBCLU}. This is critical in high-dimensional data, where clusters sometimes only exist in particular feature subspaces, and some global clustering methods might miss such latent structures. Their drawback is that they have high computational complexity.





While a wide range of clustering algorithms exist, such as DBSCAN, Spectral Clustering , Gaussian Mixture Models, and Subspace Clustering methods, this thesis focuses primarily on k-Means. This method was selected because k-Means offers a simple, efficient, and widely used baseline, allowing for robust evaluation of feature spaces within the visualization framework.
 

 


\section{Projection and Clustering Quality Metrics}
Evaluating the quality of clusters and hence the quality of the feature space, is extremely important for downstream tasks like model training and decision making. There exists methods of quantifying the quality of clusters, known clustering validity  indices, a.k.a clustering quality metrics. Cluster validity indices can be classified as internal, known as intrinsic and external, known as extrinsic, each offering a different perspective. It is crucial to understand that no single metric is universally optimal, each metric has biases and limitations of its own. It is important to remember that each metric looks at a slightly different part of “cluster quality” and no single measure is perfect for everything, especially in high dimensional scenarios, where results can often be misleading, without normalization of contextual analysis \cite{Ansari2015ClusterValidity}.
High dimensional data can also distort metrics, so it is essential to combine quantitative evaluation with visualization. \cite{Gosgens2019}

\subsection{Internal Validity Indices (Intrinsic Metrics)}
These types of metrics check the cluster quality based only on the data itself, without requiring outside ground truth labels. They were found to be very sensitive to data quality problems like blurry images and wrong labels, making them a good fit for automatic quality checks. While internal clustering metrics assess the visual separation of clusters, they often lack the ability to validate the integrity of underlying dimensionality reduction process.

\subsubsection{Silhouette coefficient}
It measures how similar a data point is to its own cluster (how close it is), compared to other clusters , (how far apart it is). It was originally proposed by Rousseeuw \cite{Rousseeuw1987Silhouette}. The values go from -1 to +1. Higher values mean better-defined clusters. Values above 0.5 are generally good, values above 0.7 indicate strong clustering, whereas negative values suggest that it is in the wrong cluster, i.e. misassignment. Finally, values around 0 indicate that there is some overlap. 

The silhouette score for a point \(i\) is calculated as:
 
\[
s(i)=\frac{b(i)-a(i)}{\max\{a(i),b(i)\}}
\]


Where \(a(i)\) is the average distance between \(i\) and all other points in its cluster, and \(b(i)\) is the minimum average distance to points in any other cluster.


While it is easy to understand, Silhouette coefficient tends to prefer round,  equally sized clusters, and it might not be as useful for clusters that are oddly shaped. One study found it to be a better indicator than Davies-Bouldin and Dunn indices \cite{SlobodanPetrovic2013}.


\subsubsection{Davies Bouldin Index (DBI)}
Introduced by \cite{DaviesBouldin1979}, it calculates the average similarity ratio of each cluster with its most similar cluster. Similarity is defined as the ratio of how spread out things are within a cluster to how far apart the clusters are. Lower values mean better separation, with 0 being the best possible value, meaning that the clusters are perfectly separate. 
DBI is defined as 
\[
R_{i,j}=\frac{S_i+S_j}{M_{i,j}},
\quad 
DB=\frac{1}{k}\sum_{i=1}^{k} \max_{j\neq i} R_{i,j}
\]

where \(S_i\) is the intra-cluster dispersion for cluster \(i\), and \(M_{i,j}\) is the distance between cluster centroids and \(k\) is the number of clusters. Lower DBI values indicate better clustering

DBI is sensitive to outliers and differences in cluster shape and density. \cite{PalBiswas1997}

\subsubsection{Trustworthiness (Dimensionality reduction fidelity)}
To fill the “trust” gap that's inherent to low-dimensional projections we use the Trustworthiness metric. Originally proposed by Venna and Kaski \cite{Venna2001}, this metric quantifies the degree to which local neighborhood structure is preserved when data is being mapped from a high dimensional space to a lower dimensional 2D projection. Specifically, it is used to measure the presence of false neighbors, i.e. data points that appear closer in visualization, but which are actually distant in the original feature space. This metric is bounded between 0 and 1, which makes for an objective comparison of different projection methods, across diverse datasets and ensures a consistent normalization. If a projection has high trustworthiness, (near 1.0) the user can be confident that the clusters that they see on the screen are real structures and not artifacts of the algorithm.

It is defined as

\[T(k)=1-\frac{2}{nk(2n-3k-1)}\sum _{i=1}^{n}\sum _{j\in N_{i}^{k}}\max (0,(r(i,j)-k))\]

Where \(n\) is the number of samples (data points).\(k\) is the number of nearest neighbors considered. \(N_{i}^{k}\) is the set of \(k\) nearest neighbors of sample \(i\) in the output (embedded) space. \(r(i,j)\) is the rank of sample \(j\) in the input (original) space, when ranked by distance from sample \(i\) (e.g., the closest neighbor has rank 1). 

\subsubsection{Calinski Harabasz Index (CHI)}
It is defined as the ratio of how spread out things are between clusters to how spread out things are within the clusters. It was first introduced in 1974 \cite{CalinskiHarabasz1974}. A higher CHI score means clusters are dense and well separated. This metric has no upper limit and it is fast to compute. It is often used to find the best number of clusters by looking at the peak value as the value of \(k\) (no. of clusters) changes.

It is defined as 
\[
CHI=\frac{BCSS/(k-1)}{WCSS/(n-k)}
\]

where \(BCSS\) is between-cluster sum of squares, and \(WCSS\) is within-cluster sum of squares. Higher values indicate better-defined clusters

However, it is not always linear or the best indicator for feature space quality, no upper limit also makes it challenging to do a clear evaluation.


 

\subsubsection{Dunn Index (DI)}
The Dunn Index aims to identify dense and well separated clusters.It is defined as the ratio of the minimum inter-cluster distance (the shortest distance between any two points in different clusters) to the maximum intra-cluster distance (the diameter of the largest cluster). It was originally  introduced in 1973 by J.C. Dunn \cite{Dunn1973}. A higher value indicates better separation and compactness.

\[
DI=\frac{\min_{i\neq j} \delta(C_i,C_j)}{\max_{k} \Delta(C_k)}
\]

Where \(\delta(C_i,C_j)\) is the distance between clusters \(C_i\) and \(C_j\), and \(\delta(C_k)\) is the diameter of cluster \(C_k\). 



Its major drawbacks are that it is sensitive to noise and outliers, which can artificially decrease the inter-cluster distance. It has a high computational complexity on large datasets, as it requires calculating numerous pair wise distances. \cite{Arbelaitz2013}

\subsubsection{S\_Dbw}
S\_Dbw is a metric that measures both the compactness and the separation of the clusters. It achieves this by combining two components, an intra-cluster variance term (\(Scat\)) that measures compactness and an inter-cluster density term (\(Dens_{bw}\)) that measures separation based on the density of points in the region between clusters. 

In research \cite{halkidi2001clustering}, it has been found to be strong across different effects and works well on datasets with noise and complex cluster shapes, it outperforms many traditional indices. Its primary limitation is its implementation complexity compared to simpler metrics.   

\subsubsection{FERM (Feature space Evaluation and Representation Method)}
FERM (Feature space Evaluation and Representation Method) is a modern metric designed to evaluate qualities of learned feature spaces for a given classification task. Unlike traditional clustering metrics, which are unsupervised, FERM uses class labels to  give a quantitative value to a data representation. It evaluates two properties, class separability (how distinct the representation of the two classes are) and class density (how tightly grouped data points of the same class are). \cite{Morais2019FERM}

This makes it particularly suitable for evaluating the feature spaces generated by Deep Neural Networks (DNN), such as ones generated for image analysis in industrial tasks, such as image feature extractors (ex. ResNet, VGG-19 and EfficientNet-B0). Where the goal is to learn a simple representation that eases the task for a downstream classifier.



\begin{table}[h]
	\centering
	\begin{tabular}{p{3cm}p{4cm}p{3cm}p{3cm}}
	\hline
	\textbf{Metric} & \textbf{What it Measures} & \textbf{Range} & \textbf{Better When} \\
	\hline
	Silhouette Coefficient & Cohesion vs separation of clusters & -1 to +1 & Higher \\
	Davies–Bouldin Index & Intra-cluster dispersion relative to inter-cluster separation & 0 to $\infty$ & Lower \\
	Calinski–Harabasz Index & Ratio of between vs within cluster variation & $[0,\infty)$ & Higher \\
	Dunn Index & Minimum inter-cluster separation over maximum intra-cluster diameter & $[0,\infty)$ & Higher \\
	Trustworthiness & Preservation of neighbor ranks in projections & 0 to 1 & Higher \\
	S\_Dbw Index & Compactness and density separation & dataset dependent & Lower* \\
	FERM & Feature space class separability & dataset dependent & Higher* \\
	\hline
	\end{tabular}
	\caption{Summary of Internal Clustering and Projection Quality Metrics}
	\end{table}
	





\subsection{External Validity Indices (Extrinsic Metrics)}

External Validity indices, evaluate the quality of clustering result by comparing it to a ground truth classification part of the data. This implies that the data should be labelled, making these types of metrics supervised. This type of comparison allows an objective assessment of how well the clustering algorithm has retained the underlying structure of the data labels. While being good tools for benchmarking, the main drawback is the reliance on pre-existing labels, making them unsuitable for many unsupervised discovery tasks where ground truth is often unknown.


\subsubsection{Adjusted Rand Index (ARI)}
This metric measures the similarity between two partitions, i.e. the clustering results and the ground truth labels provided. It does this by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in both partitions. It then corrects this Rand Index (RI) for randomness/chance, ensuring that the expected value from random clusterings is 0. It has a range of [-1, 1] where 1 indicates a perfect match and values near 0 indicate random agreement, this makes ARI highly interpretable. As noted by \cite{HubertArabie1985}, the adjustment for chance is the key advantage over the original Rand Index, which prevents inflated scores with a large number of clusters.

Given a contingency table \(n_{ij}\) for clustering labels \(U\) and \(V\), with row sums \(a_i\) and column sums \(b_j\), total \(n\):

\[
ARI = 
\frac{
\sum_{ij} \binom{n_{ij}}{2} 
- \frac{1}{\binom{n}{2}} \sum_i \binom{a_i}{2} \sum_j \binom{b_j}{2}
}{
\frac{1}{2}\left(\sum_i \binom{a_i}{2} + \sum_j \binom{b_j}{2}\right)
- \frac{1}{\binom{n}{2}} \sum_i \binom{a_i}{2} \sum_j \binom{b_j}{2}
}
\]
 

\subsubsection{Normalized Mutual Information (NMI)}
Originated in information theory, NMI quantifies the statistical information shared between clustering assignment and the ground truth labels. The score is normalized to a [0,1] range, where 1 indicates perfect correlation. A comprehensive analysis by \cite{VinhEppsBailey2010} highlights that NMI is a robust and widely used metric, but its value can be influenced by the number of clusters, and different normalization methods yield different results.

Given entropies 
\(H(U)\), \(H(V)\) and mutual information \(I(U,V)\), it is defined as:
 
\[
NMI(U,V)=\frac{I(U,V)}{\sqrt{H(U)\,H(V)}}
\]
 

\subsubsection{V-Measure}
This is an entropy based metric that combines two desirable properties, homogeneity and completeness. A clustering is homogeneous if each cluster contains only members of one class. A cluster is considered complete if all the members of a given class are assigned to the same cluster. The V-measure is the harmonic mean of these two values, providing a single, interpretable score between 0 and 1. \cite{RosenbergHirschberg2007}. This metric was introduced to address the shortcomings in other approaches that might ignore the two properties stated above.

\[
V = 2\cdot\frac{h\,c}{h+c}
\]
Where 
\(h\)
 and 
\(c\)
are normalized information-theoretic scores.


\subsubsection{Fowlkes-Mallows Index (FMI)}
FMI is defined as a geometric mean of precision and  recall. FMI computes the similarity between two clusterings by seeing the number of pairs of points that exist in the same cluster in both the partitions. It ranges from [0,1] and can be considered intuitive, however, it can be sometimes misleading. As observed in some studies, FMI assigns high scores even if the clusterings don't align with the ground truth, especially if an algorithm produces a large number of small, pure clusters. It is considered generally to be less discriminative than ARI or NMI due to this. \cite{Amigo2009}
Given true positive (TP), false positive (FP), false negative (FN):
\[
FMI = \sqrt{ \frac{TP}{TP+FP} \cdot \frac{TP}{TP+FN} }
\]



\subsection{Additional metrics}

Other useful metrics for feature space analysis also exist beyond the standard clustering indices. 

\subsubsection{No. of outliers}
This refers to the number of data points which deviate significantly from their respective cluster centers, a.k.a. Outliers. The higher the value of this metric, the worse is the quality of the feature space. This simple metric is especially useful for understanding feature space noise and data integrity.

\subsubsection{Mahalanobis distance}
Mahalanobis distance measures the distance of a data point from a data distribution, it is defined by 
For a sample 
\(x\), distribution mean \(\mu\)
and covariance \(\Sigma\):

\[
d_M(x)=\sqrt{(x-\mu)^\top \Sigma^{-1}(x-\mu)}
\]
This distance accounts for covariance structure and is very effective for doing multivariate outlier detection/ anomaly analysis. \cite{Mahalanobis1936}

Bhattacharya distance
This metric measures the overlap between two probability distributions. Higher Bhattacharya distances imply more dissimilarity. It also serves as a strong metric for evaluating feature separability and also to perform feature selection. \cite{Bhattacharya1943} \cite {BruzzoneSerpico1998}
Given two distributions 
\(p\) and 
\(q\):


\[
D_B(p,q)= -\ln\left(\sum_x \sqrt{p(x)\,q(x)}\right)
\]


Higher distances implies less overlap.
 
\subsubsection{Jefferies Matusita}
This is a normalised variant of Bhattacharya distance, whose value is bounded between 0 and 2. It is also used often in feature selection due to its intuitive scale and computational efficiency. \cite{Zhang2023JM}
\[
JM(p,q)=\sqrt{2\left(1 - e^{-D_B(p,q)}\right)}
\]



\begin{table}[h]
	\centering
	\begin{tabular}{lccc}
	\hline
	\textbf{Metric} & \textbf{Category} & \textbf{Range} & \textbf{Better When} \\
	\hline
	Adjusted Rand Index (ARI) & External & $[-1,1]$ & Higher \\
	Normalized Mutual Information (NMI) & External & $[0,1]$ & Higher \\
	V-Measure & External & $[0,1]$ & Higher \\
	Fowlkes-Mallows Index (FMI) & External & $[0,1]$ & Higher \\
	Mahalanobis Distance & Distance & $[0,\infty)$ & Lower \\
	Bhattacharyya Distance & Distribution distance & $[0,\infty)$ & Higher \\
	Jeffries-Matusita & Distribution distance & $[0,2]$ & Higher \\
	\hline
	\end{tabular}
	\caption{Summary of External and Distance-Based Clustering/Feature Space Metrics}
	\end{table}


\subsection{Need for visualization aided interpretation}
It is very crucial to note that simply relying exclusively on metrics is not enough, since they simplify complex underlying structures into a single digestible numeric value. Visualization is still essential for understanding nuance on cluster forms and context. This means that we need some kind of interactive visual exploration for gaining a full understanding of the feature space. 

The general observation is that internal metrics match well with classification accuracy and supports using these metrics as a way to guess future model performance (e.g. Silhouette scores often align with model performance), suggesting that intrinsic metrics can serve as a proxy for clustering performance when ground truth is not available, as is often unavailable in real world scenarios  \cite{Lowe2024}. This thesis acknowledges the biases and limits of individual clustering metrics. Therefore, it uses a smart approach which focuses on checking multiple metrics, and confirming them with visualization, to give a more reliable and detailed view of the feature space quality.


	
 





\section{Visualization of Feature Space and Interpretability}
High dimensional datasets and their feature spaces are hard to interpret with numbers alone, there is an underlying visual spatial structure of the feature spaces generated, that metrics might take into calculation, but they ignore the power of visualization techniques and visual interpretation which can enhance the comprehension of such datasets.

Internal clustering metrics compress rich structures into numeric scores, which usually  hide tradeoffs like local neighborhood preservation vs. global layout stability. Visualization can complement these internal metrics, by exposing patterns, distortions and outliers that affect the downstream decision making. In this thesis, the goal is to not just show low dimension projects, but to also help end users connect what they visually see in the 2D representations with quantitative, metric-based evidence. Empirical studies also demonstrate that quality metrics can be used to guide visual analyses and filter our uninteresting and cluttered views. \cite{Behrisch2018QualityMetrics}

2.4.1 Why visualization is needed alongside metrics
Quality metrics offer many valuable insights, each capturing different aspects of the feature space but they may sometimes conflict with each other on the same data. Prior research shows that no single metric can tell the whole story, since “quality” can encompass things such as clutter, overlap, pattern recognition etc. \cite{Behrisch2018QualityMetrics} This is why visual analytics is so powerful, metrics guide the whole process, but in the end, people are needed to inspect, compare and interpret the data. 

Visualization theory provides some design guidance for doing it well. Munzner's nested model emphasizes firstly clarifying data and task abstractions, then choosing the appropriate encodings and interactions and finally, validating the result. And for comparison based tasks, Gleicher's taxonomy highlights three patterns, which are, juxtaposition, superposition and explicit encoding of differences. The methods proposed in the thesis, aim to follow these guiding visualization theory principles. DimCompare does this by juxtaposing projections to compare them by showing them side by side and using annotations for clusters (explicit encoding). BarDar uses explicit encoding to show multiple quality metrics in one easy to read chart. \cite{Munzner2009NestedModel} \cite{Gleicher2011VisualComparison}  

\subsection{What static plots miss}
Static scatterplots are the way to traditionally visualize high dimensional representations, while still useful, they often hide the crucial artifacts that occur after performing dimensionality reduction, like distortions (false neighbors i.e. data points which appear closer together but are not) and tears (data points closer together in the original data are pulled apart in the plot). A body of research highlights these issues, it is argued that analysts must assess first how reliable the projection is first, before making conclusions. These studies show that different dimensionality reduction techniques often optimize for different criterias, which leads to disagreements in the resulting structure. To address this, they propose use of diagnostic overlays and  metrics to reveal errors. \cite{LespinatsAupetit2011CheckViz}  

Our system adopts this approach, we make the differences visible, highlight the unique features that drive each cluster formation, and also pair visualizations with the quantitative metrics to help prevent misinterpretation.

Traditional radar charts are difficult to interpret because if the order of the axes is changed, the shapes change for the same data and can sometimes get super noisy visually, meaning the order of the axes matters. Moreover, the shapes are hard to read, human brains aren't good at judging and comparing complex irregular shapes \cite{ClevelandMcGill1984GraphicalPerception}, it is hard to tell which polygon is smaller or bigger just by visual inspection. This makes just static radar plots unsuitable for intuitive comparative analysis.

Our system BarDar solves these issues, we use a hybrid design that combines simple radar-style overview with a separate bar chart, we tried to use an approach based on well known criticisms for radar plots. This barchart shows the aggregated score of each projection, so the user doesn't have to guess which radar chart is better and do the math in their head. 

\subsection{Visual encodings for feature spaces}
Glyphs are a common way to encode multivariate attributes at points or in regions. A recent report discusses when glyphs help and how to design them for readability and detail \cite{Borg2005MDS} DimCompare's floating cluster annotations extend this by providing cluster level summaries, which highlight what features deviate from the rest of the data, tying together the visualizations back to the original data space.This design makes use of explicit encodings and the recent literature to enhance projections with context information that is often lost when using standard methods. 

\subsection{How this thesis aims to extend existing tools}
There already exist relevant visualization tools like Embedding Projector \cite{Smilkov2016EmbeddingProjector} and Clustrophile 2 \cite{Cavallo2019Clustrophile2} which provide good support for explorations of single projections and evaluating clustering, they are not designed to enable and assist with comparative assessment and evaluation of dimensionality reduction methods. This is an important challenge for analysts who have to decide which dimensionality reduction projection best represents their data's structure.
Our thesis addresses this by creating a workflow specifically for comparative dimensionality reduction evaluation, with cluster aware explanations and multi-metric summaries, which are optimized to judge dataset “learnability.”. Our design addresses the gap noted in recent work by Espadoto et al. that call for techniques like linking projections with high dimensional space, metric quality etc. \cite{Espadoto}
 
 
\section{Foundational concepts in Interactive Visualization}
Visual exploration plays a key role in understanding high dimensional spaces and the hidden structures present inside them. This thesis makes use of several proven interactivity techniques derived from visual analytics, to make a system tailored to real world, industrial data scenarios. It especially uses ideas from Tamara Munzner's \cite{Munzner2014} “Visual analysis and design” and how Gleicher et al \cite{Gleicher2011VisualComparison} categorize comparative visualization. This theoretical base makes sure that the design choices that went into the system are not random, but are guided by best practices and aim to solve known problems in the field.

\subsection{Gleicher et al.'s taxonomy of comparative visualization}
This framework identifies three main ways to compare visualization

\textbf{Juxtaposition}   

Juxtaposition means putting visualizations side by side for direct comparison. DimCompare view with its two scatterplots which depict two different dimensionality reduction techniques is an example of this strategy. L'Yi et al. \cite{LYiJoSeo2020ComparativeLayouts} revisit comparative layout idioms including juxtaposition, superposition, and explicit encoding.
In the BarDar chart, two views help the users compare metrics for different dimensionality reduction methods, one view depicts a radar chart and another shows a bar chart representing the aggregated scores from the radar chart.

\textbf{Superposition}  

This means layering different views on top of each other, while not the main method in DimCompare, density contours can be seen as a type of superposition over the scatterplot data points. In the BarDar chart, superposition is used to overlay radar charts of different dimensionality reduction techniques on top of each other.

\textbf{Explicit encoding}  

This method involves showing the differences or extra information with special visual elements. Important data is explicitly made known to the user to reduce the cognitive load. The cluster glyphs in DimCompare are an example of explicit encoding; they show high dimensional feature differences. They represent what makes a cluster different in original high dimensional space, bridging the gap between 2D projection and basic feature characteristics. In the BarDar chart, the shapes of radar charts, corresponding to different metrics are explicitly encoded for the user, as a bar chart, since visual calculation of areas of irregular shapes is very challenging. 



\subsection{Munzner’s principles}
Our system design, both directly and indirectly follows several of Munzner's \cite{Munzner2014} key ideas for good visualization.



\textbf{Visual encoding}  

The design of the system carefully thinks about how visual elements like points, colors, glyphs and bars are used to show data properties and relationships effectively, and following good visualization practices. For example color is always used to show which cluster a point belongs to, transparency is used to manage the density of data, different colour pallets are used for separate visualisations.
 
\textbf{Interaction techniques}  
 
Munzner stresses how important interaction is for exploring data. Our system includes basic methods like selecting, linking and moving around (panning, zooming etc). In DimCompare, users can use brushing and linking to follow outliers across different projections. Tooltips are also made visible in 2D projection, when the cursor is hovered over a datapoint, which shows original high dimension features.

\textbf{Overview First, Zoom/Filter, Details on demand}
 
This idea suggests giving a general overview of the data first, letting users zoom or filter to areas they are interested in, and then providing detailed information when asked for. Our visualization framework design follows this, the BarDar chart gives and overall summary with numbers, but users can then decide to zoom in, explore and apply filters for the same data, using the DimCompare view, which also supports brushing, feature selection etc., and finally users can enable cluster annotations to get detailed information about specific clusters on demand.

\textbf{Scalability}
 
Dealing with clutter in large datasets is very important, the design choices we made, like limiting the number of bars in cluster annotation, having a density aware mode, ability to toggle visual encodings, zoom aware transparency for cluster annotation to avoid occlusion, scrollable list views for feature selection for really high dimensional datasets etc. While the current system is made for small to medium size datasets, considerations like these set the stage for future scaling to large datasets.

\subsection{Schneiderman's Mantra}
Schneiderman's \cite{Shneiderman1996Mantra} information seeking mantra, “overview first, zoom and filter, then details on demand, gives foundational guidance for the design of the interaction system. This is implemented through:

\textbf{Overview:} The Cluster feature contribution glyphs and metric summaries provide immediate understanding of overall data quality and structure

\textbf{Zoom and Filter:} Interactive exploration in DimCompare projections allows users to focus on their regions of interest. Users can also filter the features which will be shown in the Cluster annotations.

\textbf{Details on Demand:} Tooltips, visibility toggles for various visual encodings and detailed statistics provide information when needed, without cluttering the display.

\subsection{Further Concepts}

\textbf{Comparative Visualization}
The use of multiple synchronized views is common in information visualization. Gliecher et al. categorizes this as juxtaposition (side by side views), superposition (overlay) and explicit encoding of differences. DimCompare mainly makes use of synchronized juxtaposition to help users compare two projections performed using different dimensionality reduction techniques directly.

\textbf{Linking and Brushing}
This is a basic interactivity method in exploring the data, it refers to selecting a data point in one view and highlighting the corresponding point in another view, which enables tracking of clusters and outliers across different dimensionality reduction projections. Users can make a custom cluster selection by brushing over desired points. Unfortunately current tools provide little to no support for users to explore data in this way. \cite{Srinivasan2018InteractiveDataFacts}

\textbf{Glyphs and Annotations}
Using glyphs, which are compact visual summaries over clusters, fills the gap between 2D projections and original feature spaces. They are used to make scatterplots richer by giving more information about individual points or clusters. Glyphs can be used to encode anything, such as data labels of individual data points or even used for clusters to summarize key feature differences driving these clusters. 


\subsection{Human in the loop Dimensionality reduction}
A study by Sacha et. al. \cite{Sacha2017VisualInteractionDR} suggests a model for adding human interaction into the dimensionality reduction process. They found out different situations where users can guide the algorithm, like by choosing the features or tuning parameters etc. Our system fits into this model by letting the users explore dimensionality reduction results by changing features, number of clusters etc and use what they learn to drive future decisions.	

\subsection{Overplotting and density management}
A huge problem in high dimensional data scatterplots is overplotting, any high density data visualization is bound to suffer from clutter. Ways to target this include use of transparency, zooming, grouping data and changing to density based views. DimCompare provides the user with the option to show contour plots, for when dealing with high density data. These highlight the high-density areas using a 2D Kernel Density Estimate (KDE) over the points.
We also make use of semantic zooming and toggling data point visibility, the cluster annotations automatically start getting more transparent when zooming in to reveal to the viewer, points which were occluded by the annotation.

Combining these information visualization methods, like juxtaposition, brushing, glyphs, and cluttering solutions, is a good way of dealing with challenges that high dimensional data usually presents.This work addresses limitations of previous methods and supports a more interpretable, metric driven exploration of high dimensional feature spaces.
 




\section{Existing Visualization Frameworks for Comparing and Interpreting Dimensionality Reduction}
Many visualization systems have been built to help users interpret the embeddings, compare dimensionality reduction methods, and evaluate the projection quality. These systems have informed our design choices and also show what gaps remain in real situations.

\subsection{Embedding interpretation and inspection tools}
Tensorflow embedding projector is a  widely used tool, which was introduced as a web interface for exploring high dimensional embeddings, using PCA, t-SNE and UMAP, it supported search, selection and neighborhood inspection \cite{Smilkov2016EmbeddingProjector} It was helpful in popularizing interactive projection exploration for practitioners but it provides limited comparative assessment across different dimensionality reduction methods, with no support for metric overlays.


\subsection{DimReader}
DimReader is a visual interaction framework by Cavallo \cite{CavalloDemiralp2018DimReader} which focuses on explaining projections rather than just displaying them. They introduce forward and backward projection and landmarks which let users see how changes in the features move the point in 2D projection. This work is important because it links low dimension data to high dimension data which is in line with our cluster annotation/glyph idea. 

\subsection{Clustrophile2}
Clustrophile2 is also a tool by Cavallo \cite{Cavallo2019Clustrophile2}, which provides guided workflows for interactive clustering analysis, it integrates algorithm selection, parameter steering and visual diagnostics . This system shows how to mix semi automated suggestions with a human judging when choosing cluster models and parameters. Unfortunately it was made to explore single projections and still not for comparative analysis across different dimensionality reduction techniques.

\subsection{Projection comparison and quality assessment}
Projection inspector is a tool which provides interactive projection space, where users can move between projection methods and interpolate new layouts between methods, while also inspecting quality metrics \cite{Pagliosa2015ProjectionInspector}. It explains projection choice as a trade off and combines layout browsing with metric readings, which was a guide for the comparative ideas presented in the thesis like DimCompare and the metric summary in BarDar.

Distortion focused tools such as CheckViz, ProxiLens and other follow ups by Aupetit and colleagues encode false neighbors (data points that are close in low dimension but far apart in high dimension) and missing neighbors (points that are close in high dimension but far apart in the projection) and local stress directly into the visualization. \cite{HeulotAupetitFekete2013ProxiLens} \cite{LespinatsAupetit2011CheckViz}. This work argues that users should see where the projections are unusual instead of just reading a single numeric value. The DimCompare tries to follow this philosophy, qualitative and quantitative cues and showing structure in two separate dimensionality reduction views with cluster annotations.

Surveys and frameworks have helped generate and refine the ideas that went into this thesis. Nonato and Aupetits survey links dimensionality reduction techniques, distortions in structures, task and enrichment of layouts, which motivated us in adding local quality overlays and explanations  to embedding plots \cite{NonatoAupetit2018MultidimProjection}. More recently, a review by Behrisch et al., looked at quality metrics for dimensionality reduction, providing us with guidance on which metrics capture which aspects of structure preservation. \cite{Behrisch2018QualityMetrics} which back our choices to combine visual encodings with multiple metrics rather than rely on a single score.

\subsection{Human interaction and reliability}
A structured literature analysis by Sacha et al. looks at how users interact with dimensionality reduction and proposes a process model for human in the loop projection analysis. \cite{Sacha2017VisualInteractionDR} Their findings support the importance of brushing and linking, parameter steering  and explanatory views, all of which are incorporated in the design of DimCompare. A newer survey by Jeon et al. argues that reliability remains a central issue and advocates for workflows where users can “see” assumptions, uncertainty and distortions. \cite{Jeon2025ReliableVisualAnalytics}



Existing systems let users browse dimensionality reduction projections, inspect neighborhoods and overlay quality cues, but these methods fall short when it comes to cluster level explanations and side by side dimensionality reduction methods comparison with unified multi-metric summaries. DimCompare fulfills the first gap by making use of floating cluster glyphs and BarDar fills the second gap by providing compact multi metric summaries that reduce mental calculation when comparing different dimensionality reduction methods.

\subsection{What this means for our work}
We adopt a side by side dimensionality reduction comparison (DimCompare) rather than a heavy superposition to reduce the clutter and while also keeping the context.
We add cluster level feature glyphs to bridge the gap that exists between low dimension patterns and high dimensional explanations, which is a gap in many systems.
We integrate multiple quality metrics and summarize them visually (BarDar), this along with explicit encodings reduce the cognitive load and over reliance on any single index.
We acknowledge distortions and reliability concerns from prior work in this field and address them using brushing, linked selection and optional density encoding, which help reveal distortions like missing and false neighbors. This boosts interpretability and exploration in visual analysis.

 

 
\section{Chapter summary} 
This chapter establishes the theoretical foundations for the thesis by reviewing the literature across high-dimensional data analysis, clustering, quality metrics and interactive visualization. The key findings of the review are:
\begin{itemize}
\item \textbf{Review of dimensionality reduction}: Analyzed the fundamental trade-offs between key dimensionality reduction methods (eg. linear PCA vs non linear UMAP), establishing the projection instability problem where different algorithms reveal different, and sometimes even conflicting, data structures.
\item \textbf{Survey of clustering and quality metrics}: Detailed the various internal metrics like Silhouette, Davies Bouldin, S\_Dbw and external metrics like ARI, NMI etc used to quantify the feature space quality, providing the vocabulary/language used by the BarDar and DimCompare tools.
\item \textbf{Analysis of Visualization Principles}: Grounded the system's design in established frameworks, including the Gleichers taxonomy for comparative visualization, justifying the DimCompares juxtaposed views, and Munzner;s nested model for the interaction design of the tools.
\item \textbf{Identification of Research Gap}: Surveyed existing visual analytic tools like Embedding projector, Clustrophile 2, concluding that a clear gap exists for systems that, at the same time, supports side by side dimensionality reduction comparison, multi metric dashboards and feature level cluster explanations. 
\end{itemize}

This review confirmed the need for novel tools presented in this thesis and provided the theoretical and research basis for their design, which is detailed in the next few chapters.

\pagebreak

\chapter{Methodology}
 

The previous chapters pointed out the need for more effective tools to evaluate the quality of high dimensional feature spaces. The limitations of plots and abstract quantitative metrics, as discussed earlier, need a solution that closes the gap between algorithm output and human interpretability. This chapter goes into the design, architecture and core methodologies of the visual analytics system we developed to tackle these issues. The system, called DatasetWiz, is created to tackle the research questions proposed in this thesis. It is a robust analytical backend and a frontend with novel and interactive visual components. The design process was guided according to established principles in visualization and visual analytics.

\section{Industrial Context and Collaboration}
The idea for this visualization tool came after an industrial collaboration with Robert Bosch GmbH, that provided the real world context, datasets, and domain expertise essential for this research. This partnership highlights the significant value of combining industry expertise with academic inquiry to solve practical challenges, and advance our fundamental understanding of visualization and data analysis.

\subsection{Bosch Manufacturing Quality Assessment}
Robert Bosch GmBH is a leading global supplier of technology and services, with automotive technology representing its largest business sector, the company's focus on quality is reflected by their investments in advanced hardware manufacturing and quality inspection. In modern manufacturing environments, quality assessment must handle intricate objects while also sustaining the high volume and consistent reliability.

The company's quality assessment challenges span across multiple domains, including automotive components which need to have accurate detection for mechanical defects, dimensional variations and material inconsistencies. Consumer products must also meet strict aesthetic and functional standards. All of these requirements create a need for flexible analytical tools that can adapt to different products and defects.


\subsection{Current Quality Assessment Workflow}
Bosch's existing quality assessment workflow combines traditional inspection methods with advanced computer vision systems. Human inspectors handle complex subjective assignments, while automated systems process most of the high volume of normal samples based on many features such as objective measurements of dimensions, type of material, which machine the part was associated with, manufacturing settings etc. These type of datasets are high dimensional; sometimes, the dataset included just the pictures of the parts, which were converted into vectors by using image feature extractors, based on deep neural networks.

These high dimensional datasets of the industrial hardware parts, are put through dimensionality reduction process, and projected onto a 2D space. This dimensionality reduction is an essential part of the quality analysis process as it gives the users simplified visualized information about the parts, as in how many clusters form, which points deviate from their clusters, which usually implies some defects, mislabelling, outliers etc, as these  high dimensional datasets are hard to visualize. Cluster shapes also give insight into the effects happening on the real world parts like blurry images.

These datasets’ feature spaces are evaluated for quality using a combination of clustering metrics, in addition to manual 2D projection inspection, to judge whether these datasets can be used to train machine learning classifiers downstream in the pipeline.They found out that generally a dataset with good cluster separation correlated to good accuracy for the machine learning classifier.  However, their current approach was limiting because the interpretation of these results by non-technical users was still challenging, and even technical users needed to know what exactly a numeric clustering metric meant. Apart from the understanding of each metric, the intricacies of each dimensionality reduction technique should also be known by the user as projections might look wildly different according to the dimensionality reduction technique used.

This was their current approach and they were interested in evaluating the quality of feature spaces more. They wanted to find out additional metrics which can be used to evaluate the quality of these feature spaces and 2D projections, and this was the main focus of the collaboration with Bosch.



\subsection{Collaborative framework}
The collaboration with Bosch was structured to address both the practical needs and longer term research objectives. Johannes Mohren and Dr. Sabrina Schmedding served as the primary industry contacts, providing domain expertise, dataset access and validation of research directions. Regular meetings ensured that the research remained grounded in practical requirements while still pursuing novel visualization approaches.

The partnership provided access to real manufacturing datasets, that would be impossible to replicate in academic settings, while the industry partners benefited from state of the art visualisation research that could improve their quality assessment capabilities. This kind of mutually benefiting model enabled the development of tools that are both academically novel and practically relevant.

\section{Task Analysis}
Through interviews and observations with academic users and industrial real world users (Bosch quality assessment team), these core tasks were identified.

\begin{itemize}

\item \textbf{Algorithm comparison :} Users need to compare different dimensionality techniques effectiveness for their specific datasets, understand differences and tradeoffs between global and local structure preservation of these dimensionality reduction algorithms.

\item \textbf{Pattern Discovery :} Identification of meaningful clusters and outliers in high dimensional feature spaces,  with emphasis on which exact features are affecting the groupings. Ex: with the industrial users, they wanted to know which features were causing anomalous clusters in the high dimensional space.

\item \textbf{Quality Assessment :} Evaluation of clustering and feature space quality of the high dimensional dataset, using multiple quantitative metrics. These metrics helped users evaluate the suitability of the dataset for downstream tasks such as training a Machine Learning model on the dataset. The metrics also reduced cognitive load, by converting the complex 2D projection into an easily digestible number.

\item \textbf{Knowledge Communication:} Sharing these insights gained, across multiple teams and organisational levels, with technical or non-technical backgrounds, using easily interpretable visualisations.
\end{itemize}

\subsection{Current Practice}
The initial phase of this work was looking at the current industrial workflow and to perform a requirement analysis, keeping in mind the challenges observed in industrial data science workflows. The primary target users were data scientists and ML engineers, who are tasked to assess the dataset quality prior to training. We did an analysis of their existing practices and limitations, and identified several key user requirements. Bosch's old approach was modelled after existing traditional analysis approaches, it primarily relies on
\begin{itemize}
\item \textbf{Traditional statistical analysis :} Basic statistical summaries and correlation analysis, which provide limited insights into complex structures of these datasets and image feature extractor derived feature spaces.   

\item \textbf{Single method visualization:} Individual dimensionality reduction techniques, typically only t-SNE and PCA, are applied to the dataset without systematic comparison, or validation of the results.  

\item \textbf{Metric based evaluation :} Clustering quality is assessed using individual numeric metrics, without comprehensive comparison or visualization of trade offs between different approaches.  

\item \textbf{Manual interpretation :} Quality engineers must manually interpret the complex algorithmic results, without visualisation support, leading to potential misunderstandings and possibly suboptimal decisions.
\end{itemize}

\subsection{Identified limitations}
Several critical limitations emerged after analysing, such as
\begin{itemize}
\item \textbf{Need for Comparative Analysis :} Users need a way to understand the impact of different dimensionality reduction algorithms on their data, since no single method is universally optimal and cannot help understand their relative strengths and limitations for specific datasets.  

\item \textbf{Need for Feature level explanation:}  Dimensionality reduction algorithms have a black box nature which was a core challenge. Users needed insight as to why some clusters are formed, what does this protrusion in the cluster mean for their real world data and their implications etc. Static plots have no connection back to their original data, and users had a hard time figuring out which outlier on the plot corresponded to which data point.  

\item \textbf{Need for Integrated Visual-Metric validation :} Visual inspection alone was not enough, and sometimes deemed to be too subjective. Multiple quality metrics were being calculated independently without visualization tools that can reveal relationships and trade offs. Metrics alone provide quantitative evaluation and validation but do not show the whole picture. In the real world data is often mislabeled, therefore even with ground truth, the metrics might fail to represent some outliers, while visual inspection can highlight these outliers.  

\item \textbf{Need for a Unified, Interactive workflow :}  A significant pain point for the users was that they had to rely on separate, disconnected tools, e.g. separate python scripts for preprocessing, embedding, dimensionality reduction and visualizations. A primary requirement was a single, unified, seamless web based interface, that integrated the entire analytical pipeline, from data upload to final insights, while still being accessible to non-technical users.  

\item \textbf{Scalability constraints :} Existing tools struggle with size and complexity of industrial datasets, and hence require manual processes that are time consuming and error-prone.
\end{itemize}

\subsection{Requirements for improvement}
Based on the collaboration discussion and analysis of current limitations, we identified some key requirements that emerged  


\begin{itemize}
\item \textbf{Comparative analysis:}  Tools must enable side by side comparison of different dimensionality reduction techniques with some way of emphasizing similarities or differences.

\item \textbf{Feature interpretability:} Visualization should reveal which features contribute most to the clustering patterns, allowing users to understand various drivers of different groupings.

\item \textbf{Integrated Metrics:} Multiple clustering quality metrics should be  presented to users, in a unified framework, which enables the user to understand trade offs and relationships.

\item \textbf{Interactive exploration:} Users need the ability to explore various perspectives of/on the data, via panning, zooming, brushing and filtering operations.

\item \textbf{Scalable performance:} Tools should be able to handle industrial scale datasets with thousands of samples, each with hundreds of features, with acceptable response times.

\item \textbf{Intuitive user interface:} The interface must be accessible not only to data scientists, but also to people who are not familiar with methods like dimensionality reduction like quality engineers, students, ML engineers etc who have varying levels of technical expertise.

\end{itemize}



\section{Technical requirements for visualization tools}
\subsection{Functional requirements}
\begin{itemize}
\item \textbf{Multi-Technique Comparison :} Ability to compare results from different dimensionality reduction methods like PCA, t-SNE, UMAP and MDS simultaneously, enabling users to to understand how their choice of algorithm affects the data representation.

\item \textbf{Cluster annotation :} Visual annotation of clusters, with some information about their differentiating characteristics, particularly which features differentiate each cluster from the other clusters.

\item \textbf{Interactive exploration :} Support for abilities like brushing, selection, zooming and filtering operations, that enable the detailed exploration of the dataset and its subsets.

\item \textbf{Metric Integration :} Comprehendable presentation of multiple clustering quality metrics with visualisation of their relationships and what it implies for technique selection.

\item \textbf{Export and documentation :} The ability to export 2D projections and other visualisation/analysis results for including in the reports and presentations, helping users communicate their findings with other stakeholders.

\end{itemize}


\subsection{Performance requirements}
Industrial applications call for some specific performance constraints
\begin{itemize}

\item \textbf{Response time}: Interactive operations like brushing zooming and selection must provide immediate visual feedback (<200ms) to support fluidity in exploration. The time required for the actual dimensionality algorithm will vary according to the size of the dataset and the compute hardware used.

\item \textbf{Dataset Scale :} Tool must handle dataset with 1000-10,000 samples and 10-5000 features with minimal performance degradation.

\item \textbf{Memory Efficiency :}  Browser based tools typically must operate with memory constraints while handling bigger datasets and complex visualizations.

\item \textbf{Concurrent Use :} The system must support multiple users accessing different datasets  simultaneously, without interference.
\end{itemize}



\subsection{Usability requirements}
The target use base includes quality engineers, data scientists, students and domain experts who all have a varying level of analytical skillset:
\begin{itemize}
\item \textbf{Intuitive interface:}  The interface must be learnable by the users without extensive training in advanced analytical techniques or visualization tools. The UI must make sense without any prior explanation or self-explanatory, and be visually simple, while incorporating modern web design elements, which is a design language that users tend to be familiar with.

\item \textbf{Details on demand :} Complex functionality must be organized in such a way that supports both casual exploration and detailed analysis when needed, without overwhelming the new users.

\item \textbf{Contextual help :} Documentation and tooltips must be integrated into the UI and should provide guidance to the user on interpretation of visualizations and metrics.

\item \textbf{Error Prevention :} The interface must account for common errors and prevent them, and provide clear feedback to the user about the state of the system, communicate when user actions can not be completed, loading states etc.
\end{itemize}


\subsection{Integration requirements}
Industrial deployment requires integration with existing  workflows and systems: 
\begin{itemize}
\item \textbf{Data format compatibility :}  Support for standard industrial data formats (such as simple CSV and Excel) without having to perform complex data transformations.

\item \textbf{Technology Compatibility :} Browser based deployment to minimize installation, maintenance while ensuring the system is compatible with existing corporate IT infrastructure.

\item \textbf{Benchmarking :} Comparison with existing analysis approaches both academic and industrial, to demonstrate improved effectiveness.
\end{itemize}

The requirements identified through this industrial collaboration form the basis for the design and implementation of the DimCompare and BarDar visualisation tools described in the following sections. The combination of needs of practical industry with research opportunities in visualization creates an ideal context for development of tools that advance both the academic understanding and industrial practice.

\section{Iterative Design Process}
The development of DatasetWiz followed an iterative and user centered design process. It began with creating initial low-fidelity mockups for the two primary views, i.e. DimCompare and BarDar. These wireframe mockups were then developed into a series of functional prototypes. Each prototype was evaluated informally, by "thinking aloud” with peers from the data science and visualization domain. This iterative feedback loop was crucial in refining the systems interaction design, visualization choices, visual encodings and the overall workflow. For instance, early feedback  led to replacement of simple tooltip on clusters with the more informative and expressive “Cluster Annotation” which includes feature contributions.

\textbf{Phase 1  } Domain analysis: Extensive research for user requirements, pain points amongst peers and also an extensive collaboration with Bosch industry experts to understand current workflows, identify limitations and see the types of insights needed for high dimensionality data analysis.  

\textbf{Phase 2  } Initial prototyping: Development of basic prototypes, for DimCompare as well as BarDar chart concepts, focusing on technical feasibility and core functionality.  

\textbf{Phase 3  } Iterative refinement: Multiple cycles of  prototype enhancement, based on user feedback from both the Bosch industrial partners and academic advisors, with the importance of interaction design in mind, while making the system optimized for performance.  

\textbf{Phase 4  } Validation and Documentation: Comprehensive testing with real datasets, user feedback collection and systematic documentation of design choices and the reasoning behind them.




FIG . [ADD FIGURE OF MOCKUPS ADOBE XD]


\section{Theoretical  framework}
The design of DatasetWiz is based on a user centric, iterative process that is inspired by the practical requirements of industrial data analysis, while keeping the foundations of information visualization in mind. The main objective was to move away from a passive observation of a dimensionality projection and instead actively engage in the discovery process, guided by our system.

The systems design is built on the two key theoretical frameworks discussed in Chapter 2:

\begin{enumerate}
\item \textbf{Gleicher et. al's Taxonomy of comparative visualization :} To address common challenges faced during dimensionality reduction, such as projection instability and algorithm bias (RQ2 and RQ4), our system relies heavily on Juxtaposition. The DimCompare view places two distinct dimensionality reduction projections side by side, enabling users to do direct visual comparisons. Further, the cluster glyphs are a form of explicit encoding, as they superimpose additional high dimensional information, directly onto the 2d space, to reveal what exactly drives cluster formation (addressing RQ1).
Gleicher et. al's Taxonomy of comparative visualization : To address common challenges faced during dimensionality reduction, such as projection instability and algorithm bias (RQ2 and RQ4), our system relies heavily on Juxtaposition. The DimCompare view places two distinct dimensionality reduction projections side by side, enabling users to do direct visual comparisons. Further, the cluster glyphs are a form of explicit encoding, as they superimpose additional high dimensional information, directly onto the 2d space, to reveal what exactly drives cluster formation (addressing RQ1).
\item \textbf{Munzner's Nested Model for Visualization design :} The system's entire workflow follows Munzner's principles. The BarDar chart provides the high-level overview of clustering across multiple dimensionality reduction algorithms. .The users can then proceed to the DimCompare view, to zoom and filter on specific clusters, or regions of interest using interactive brushing and panning tools. Finally, the cluster glyphs provide details on demand, revealing high dimensional deviations in the features, for any of the selected clusters.
\end{enumerate}

By consciously applying these principles, our system aims to provide a more structured and effective exploratory analysis experience, as compared to traditional, static and disconnected tools.




\section{DimCompare}
The DimCompare view is the central component of visual data exploration with DatasetWiz. DimCompare is a novel, dual-view system designed specifically to address the challenges of projection instability, information loss and the need for feature level interpretation, all of which were identified in the user requirement analysis.

\subsection{Design Rationale}
The design rationale for DimCompare was to create an environment, where users can directly and interactively compare the outputs of different dimensionality reduction algorithms. As established in Chapter 2, methods like PCA, t-SNE and UMAP have different mathematical basis and biases. This causes them to preserve different aspects of the data's structure. The primary goal of DimCompare is to make these differences transparent to the user and explorable, so that the users can build a more robust and holistic mental model of their dataset, by observing it from multiple perspectives.

\subsection{Design Goals}
DimCompare was designed to address these specific goals


\begin{enumerate}
\item \textbf{Simultaneous comparison :} enable side by side visualization of different dimensionality reduction technique results on the same dataset, allowing immediate visual comparison of  the algorithm outcomes
\item \textbf{Feature Interpretability :} Provide clear visualization of which features contribute the most to the formation of a cluster in each dimensional representation, closing the gap between high dimensional analysis and human understanding.
\item \textbf{Interactive exploration :} Support fluid and intuitive interaction patterns that allow users to explore specific data subsets and understand relationships between the different views and finally validate algorithm results through parameter manipulation.
\item \textbf{Metric Integration :} Combine qualitative visual assessment with quantitative clustering, i.e. quality metrics, providing users with multiple angles on technique performance.
\item \textbf{Scalable performance :} Maintaining interactive, almost real-time response times on high dimensional industrial-scale datasets, while also providing analytical capabilities.
\end{enumerate}

\subsection{Need for comparative analysis}
Traditional approaches to dimensionality reduction analysis focus on using a single technique, so users lack a systematic method of comparing different algorithmic approaches. This limitation is problematic in industrial contexts, where the choice of a dimensionality reduction method  has significant impact on downstream tasks, such as training a Machine Learning model on it.

The challenge is made worse by the fact that different techniques may reveal different aspects of data structure, ex: PCA preserves global relationships but it may miss out some non-linear patterns. Whereas t-SNE is good at revealing local cluster structures but it can distort global organization [. UMAP attempts to balance both global and local preservation \cite{Wani2025DRReview}. Without comparative visualization tools, practitioners resort to trial and error approaches or rely on a singular quantitative metric that might not capture full complexity of the dimensionality reduction techniques performance.


\subsection{DimCompare Features}

\subsubsection{Dual-View architecture}
DimCompare uses a side by side, juxtaposed layout to display two different Dimensionality reduction projections of the same dataset, at the same time. The interface employs a horizontal dual-view layout with two primary scatterplot panels positioned side-by-side. This arrangement supports natural left to right comparison patterns \cite{Gleicher2011VisualComparison} while maximizing the available screen space for detailed visualization. 

Each panel displays the results of different dimensionality reduction techniques applied to the same high dimensional dataset. The data points are consistent across both views and enable direct comparison of how different algorithms represent the same information, revealing technique specific patterns. Juxtaposition is a simple way to implement a comparative analysis of two different dimensionality reduction techniques, compare their projections and individual clustering tendency.

Figure 5.1: [PLACEHOLDER: Screenshot of DimCompare dual-view interface showing PCA (left) and t-SNE (right) projections of an industrial dataset with cluster annotations]


\subsubsection{Integrated metric Visualization}
Scatterplots and glyphs allow for a more qualitative exploration, complementing that, DimCompare integrates quantitative feedback directly underneath each projection view. For each of the dimensionality reduction methods (eg. PCA in View1 and t-SNE in View 2), a dedicated bar chart summarizes the clustering quality metrics calculated specifically for that projection.

\textbf{Purpose :} These charts provide instant, quantitative validation for the visual structures that the user observed in the scatterplots, directly addressing our research goal. Users can instantly see if the visually different clusters in a t-SNE plot correspond to a high Silhouette score, or if the more overlapping structure in  a PCA plot results in a poor Davies Boulding score etc, without having to do the mental visual calculations of two similar looking projections.

\textbf{Metrics Displayed :} The bar charts display the same set of internal validity indices, used later in the BarDar View, which are also the ones discovered to capture different aspects of cluster and feature space quality. These are : Silhouette Coefficient, Calinski Harabasz Index and  the recent S\_Dbw index.

\textbf{Calculation and normalization:}  It's important to note that the metrics are computed based on the clustering assignments from KMeans applied to the 2D projected data of the specific Dimensionality Reduction method shown in the panel. This allows us to directly assess the quality of the visual clustering that's presented to the user. Similar to BarDar view, we also normalize the scores in DimCompare view, to a common range of [0,1], and the metrics where lower values are better (like Davies Bouldin and S\_dbw) are inverted, to ensure that taller bars consistently indicate better quality, according to that specific metric, adding to the intuitiveness of the design.

\textbf{Contextual Information :} Hovering on the bars of the chart, tooltips appear that show the Normalized and also the Raw metric value. There are also brief textual descriptions below the metric names, that clarify what each metric measures specifically (eg. "Cluster Separation“ for Silhouette, “Cluster similarity” for Davies-Bouldin etc). This adds further intuitiveness and lowers the technical knowledge bar that's required to use the tool.

This tight knit integration of visual representation and quantitative metrics within each panel of  the DimCompare view allows for users to make more informed judgements about the quality and the trustworthiness of each of the Dimensionality Reduction projection.



\subsection{Coordinated Views Implementation}
The dual view architecture is supported by the shared data model, this enables tightly coupled interactions. The two scatterplots are coordinated through several mechanisms

\textbf{Brushing and linking :} The two views are linked with each other with interactive brushing. When a user selects a group of points from one scatterplot using the brush, the corresponding points are immediately highlighted in the other. This interaction is critical for addressing RQ4, since it allows the user to track how specific clusters, outliers and structures are represented across various dimensionality reduction algorithms. For example the user can see if a cohesive singular looking cluster in a UMAP projection is subdivided into multiple smaller groups by t-SNE, or how outliers behave under different projections.  

\textbf{Synchronized color coding :} Data points representing the same samples are colored the same across the two views, which enables immediate tracking of individual samples or groups across different dimensional representations.

 
\textbf{Figure 5.42:} The DimCompare dual-view interface, showing a PCA projection (left) and a t-SNE projection (right) of the same dataset. A brushed selection in the left view is highlighted in the right view, demonstrating the linked interaction.


\subsubsection{Visual encoding decisions}
Numerous critical visual encoding choices enhance the efficacy of the dual view comparison.

\textbf{Point representation :} Individual data points are represented as semi transparent circles, which help to reduce visual clutter. The points also are highlighted with an outline when the user hovers on it to display tooltips, which help the user see which exact data point is being investigated, especially in dense clusters with overlapping points.

\textbf{Color Coding :} A color palette is used to encode the cluster membership, with use of consistent colors across both views, which enables an immediate comparison of patterns. The palette was selected to give priority to perceptual differentiation and accessibility for the colorblind users.

\textbf{Density representation :} Due to the nature of high dimensional datasets, the scatterplots tend to be very visually dense, in such cases, it can be helpful to enable the density view which shows colored contour lines for clusters, instead of points, which helps approximate general shapes and gain additional insights.


\subsection{Feature contribution glyph}
The most novel element of the DimCompare view is the Feature Contribution Glyph aka Cluster Annotation, it's an information dense visual summary of a cluster, that was designed to overcome the inherent information loss that comes with using dimensionality reduction methods. It directly addresses the RQ1 (How can visualization reveal which features drive cluster formation?). When a clustering algorithm is applied, a cluster glyph or cluster annotation is generated and displayed for each identified cluster.

\textbf{Calculation:} To see what makes clusters different, the cluster annotation visualizes the statistical deviations of the points inside the cluster, to the rest of the dataset. For all the user-selected high dimensional features, it calculates two values. (i) The mean of that feature for all data points within the selected cluster and (ii) The mean of  that feature for all the data points outside of the cluster ( the "global background”). The percentage difference between these two means is calculated.

  
\paragraph{Cluster–background deviation.}
Let \(X \in \mathbb{R}^{n \times p}\) be the data matrix, \(C \subset \{1,\dots,n\}\) the index set of the selected cluster, and \(B = \{1,\dots,n\}\setminus C\) the background. 
For feature \(j \in \{1,\dots,p\}\), define the cluster and background means
\[
\mu_{Cj} \;=\; \frac{1}{|C|}\sum_{i \in C} x_{ij},
\qquad
\mu_{Bj} \;=\; \frac{1}{|B|}\sum_{i \in B} x_{ij}.
\]
The (signed) percentage deviation visualized by the glyph is
\[
\Delta_j \;=\; 
\begin{cases}
100 \times \dfrac{\mu_{Cj} - \mu_{Bj}}{|\mu_{Bj}|} & \text{if } \mu_{Bj} \neq 0,\\[6pt]
0 & \text{if } \mu_{Bj} = 0 \text{ and } \mu_{Cj} = 0,\\[4pt]
100 & \text{if } \mu_{Bj} = 0 \text{ and } \mu_{Cj} \neq 0.
\end{cases}
\]
In vector form for a set of user-selected features \(S\), let \(S_0 = \{j \in S : \mu_{Bj} = 0\}\) denote features with zero background mean. Then
\[
\Delta_j \;=\; 
\begin{cases}
100 \cdot \dfrac{\mu_{Cj} - \mu_{Bj}}{|\mu_{Bj}|}  & j \in S \setminus S_0,\\[6pt]
100 \cdot \mathbf{1}[\mu_{Cj} \neq 0] & j \in S_0.
\end{cases}
\]

\textbf{Visual encoding :} The percentage difference is encoded as a compact bar chart within the glyph. A bar extending upwards from the central horizontal axis indicates a feature whose average value is higher within the cluster compared to all the other data points. Whereas a bar extending downwards indicates a feature with a lower average value. The length of the bar is directly proportional to the magnitude of this percentage difference, allowing users to immediately identify the most discrimination features of that specific cluster. 



	 


\subsection{Feature Contribution Glyph Visual design}
Each annotation consists of a compact, regular visualization positioned near the centroid of each cluster. A cluster annotation contains the following:
\begin{enumerate}
\item \textbf{Feature Bar Charts :} Horizontal bars represent the top 3-5 most distinguishing features for that cluster, with the length of the bar used for encoding the magnitude of the difference from the global mean of that feature. 
\item \textbf{Directional Encoding:} Bars extending up from the horizontal baseline (x-axis)  indicate above average feature values, whereas the downward bars indicate below-average values, providing immediate directional information/encoding. For extreme cases, when a bar extends above the cluster annotation box due to high or low values, we add a pointed cap on it to indicate that it goes above/below the bounds for the cluster annotation box.
\item \textbf{Percentage Labels:} Actual numeric percentage differences are displayed alongside each bar, which enable for a quantitative assessment of feature contributions.
\item \textbf{Color Coding:} An intuitive color scheme, green for above average and red for below average, allows for rapid pattern recognition and reinforces the directional encoding of the bars.
\item \textbf{Sample size :} This is a small indicator on top of each cluster annotation which shows the size of the cluster, eg. n=184, where 184 are the number of data points belonging to that cluster. This helps users get a clearer idea of what the cluster contains
\end{enumerate}


[FIG of individual cluster annotation]

\subsection{Spatial Placement Rationale}
The feature contribution glyphs are positioned at the centroid of each of the clusters rather than in a separate list or sidebar. This choice was made since it maintains the spatial context between the 2D projection and the high dimension feature explanation. This selected layout adheres to visualization principles and coordinated multiple views, by allowing the user to inspect the statistical "why”s of the cluster without needing to move away their gaze from the visual structure that they are currently exploring. To reduce potential visual clutter, we implemented zoom level depended transparency, where the annotations fade as user zooms in to inspect the individual data points. 

\subsection{Dynamic Glyph Updates}
The visual annotations dynamically update in response to the user's interactions

\textbf{Selection Based :} When the user selects the subsets of data points from the projection using brushing, temporary cluster annotation appears on the selection, showing the feature characteristics of the selected subset, compared to the rest of the data.

\textbf{Real Time calculation :} Feature statistics are calculated in real time, according to changes in the users selection area, allowing for immediate feedback about the characteristics of different regions/ points of interest.

\textbf{Contextual Comparison:} The cluster annotations display comparison between the selected cluster and the rest of the dataset (global), between two clusters and also between the selection and the remaining data, depending on user interaction.                              







\subsection{Interactive features}
DimCompare also has a whole suite of interactive tools to help reduce visual clutter, and support a fluid analysis workflow. 

\textbf{Visual Toggles :} Users can independently toggle the data point visibility,  density contours (visualized using a 2D Kernel Density Estimate) and the cluster annotations themselves. These features allow the users to customize the view for different analytical tasks like finding the high level patterns using the density contours first, then perform detailed inspection of individual points by enabling the data points visibility.  

\textbf{Feature selection :} A dedicated panel displays the features in the dataset, users can interactively select which of the high dimensional features are included in the Feature Contribution Glyph. The features are first ranked   

\textbf{Selection based updates :} When a user selects a subset of data points through brushing or clicking, temporary annotations appear on that subset of data that show feature characteristics of user selection compared to the rest of the data, supporting investigative exploration.  

\textbf{Shift + Drag Brushing activation :} Brushing is activated using shift+drag, this is done to ensure that users dont accidentally initiate brushing with a single click. 

\textbf{Real time calculation :} Feature statistics are recalculated in real time whenever users decide to change selection regions etc, which enable immediate feedback about the characteristics of different subsets of data which the user is interested in.


 
\subsection{Implementation details}

The following JavaScript snippet illustrates the calculation of the percentage deviation $\Delta_j$ and the subsequent rendering of the glyph for the top five features:  
\\ 

\begin{lstlisting}[language=JavaScript, caption={Glyph calculation and rendering logic.}]
// Simplified glyph calculation and rendering
function updateClusterGlyph(clusterId, features, clusterData, globalData) {
    const featureImportance = calculateFeatureImportance(
        clusterData, globalData, features
    );
    
    const topFeatures = featureImportance
        .slice(0, 5)  // Top 5 most important features
        .map(f => ({
            name: f.name,
            difference: f.clusterMean - f.globalMean,
            percentage: ((f.clusterMean - f.globalMean) / f.globalMean) * 100
        }));
    
    renderGlyph(clusterId, topFeatures);
}
\end{lstlisting}
 



\subsection{Zoom and Pan operations}
Dimcompare supports multimodal aka multi input navigation, which allows for different exploration patterns inside the 2D projection, which usually are not supported in traditional dimensionality reduction workflows, where images are rendered as images though a visualisation library like matplotlib.
\begin{itemize}
\item \textbf{Mouse wheel zoom:} Standard scroll wheel zooming enables rapid change of focus with the cursor position used to orient the zoom center.

\item \textbf{Middle mouse pan :} Middle mouse press and drag provides pan operations without switching of modes

\item \textbf{Touchpad controls :} In addition to the mouse, touchpad controls are also supported in DimCompare, users can pinch in and out to zoom and a three finger drag to pan around the projection.

\item \textbf{UI Button controls :} Explicit zoom in/out buttons are also provided for alternate navigation, users who prefer interface based controls and accessibility.

\item \textbf{Independent View navigation :} Each of the 2D projection views can be navigated independently, which allows for exploration of technique  specific patterns and still maintain the ability to compare overall structures.
\end{itemize}

\subsubsection{Toggle controls}
Multiple display options enable customization based on the user's analysis needs, these toggle options also help in reducing clutter, which is a common problem when dealing with high dimensional CSV datasets, as discussed in Chapter 2.

\begin{enumerate}
\item \textbf{Cluster glyph toggle :} Users can show or hide cluster glyphs, to focus on overall patterns or detailed cluster characteristics.

\item \textbf{Density toggle :} The colored concentric density visualizations can be turned on or off, these density representations help users get an overview of the structure of the clusters, because otherwise, individual points can clutter easily and start to overlap at some specific zoom level.

\item \textbf{Point Display Toggle :} Individual points can be hidden to focus on the densities, cluster level patterns, this is particularly useful in the cases where the dataset is large and the point density creates visual clutter

\end{enumerate}


[ADD FIGURE]
\subsection{Detailed information display}
DimCompare provides many mechanisms to provide users access to detailed information, which is in accordance with the Schneidermanns mantra,  which says 'Overview first, zoom and filter, then details on demand'

\textbf{Hover tooltips:} Mouse hovering over the individual data points, displays additional information about the sample, including the original feature values for selected features and also the cluster membership of the displayed point.

\textbf{Bar chart hover :} In the metrics bar chart shown below the dimensionality reduction projections, hovering on the individual metric bars shows the real and normalised values allowing users to have a more in-depth look

\textbf{Feature selection :} The feature selection tab, which is arranged according to feature variation, also shows more information when hovered over it like rank, raw value, log scaled value etc, which is helpful when deciding which features to select for visualisation.




\subsection{Design Validation and user feedback}
The final designs for DimCompare (and also BarDar) visualizations were not arrived at in a single simple step. They are the result of an iterative design process, guided by feedback from weekly academic supervision, expert consultation with industry partners at Bosch, and an analysis of the preexisting principles in visualization research.

The DimCompare view evolved from a simple scatterplot at first, to an interactive dual view analytical tool environment. It was shaped by these primary design challenges and feedback


\textbf{Challenge 1 Explaining cluster formation (the "Why")}  

\textbf{Initial problem :} A standard dimensionality reduction scatterplot can show that clusters exist (ii.e. The "what), but it cannot and does not explain why they formed. The link to the original high dimensional feature space is also lost. This was also a core requirement of our initial research questions.

\textbf{Initial design and feedback:} Early mockups of the dimcompare have a cluster annotation which is explored using arrow based glyphs to show cluster deviation. Feedback from supervisors indicated that comparing the lengths and and directions of the multiple arrows was confusing, perceptually difficult and looked cluttered. The arrow direction was meant to encode if the feature is more than average feature values for other clusters or less than using up and down pointing arrows.

\textbf{Solution :} We iterated the cluster annotation design, this component is a compact bar chart that is placed at the centroid of each cluster. This bar graph annotation is used to explicitly visualize the statistical deviation (percentage difference) of that cluster's feature means
compared to the global average of the entire dataset. This directly addresses the question of 'what makes this cluster different from the other ones'
\begin{itemize}
\item Instead of arrows, we use bars that extend above the 'average' horizontal baseline, these bars going above this indicates higher than average value within that cluster
\item Bars extending below this baseline indicate lower than average feature values
\item The length of the bar is proportional to the magnitude of this deviation, this makes the most important features immediately apparent. \\  

\end{itemize}

\textbf{Challenge 2 Visual Clutter and Annotation placement}  

\textbf{Problem:} with multiple clusters, the new annotations would inevitably overlap each other, obscuring the data points as well, especially when zoomed out a lot.
\textbf{Feedback :} Placing annotations in a separate list was a suggestion (by research assistant Anja) that would make them easier to compare but would break the spatial link to the plot. Placing them in the centroid (suggested by Prof. Dr. Heinzl) maintained the link but worsened the clutter.  

\textbf{Solution (hybrid approach) :} We retained the centroid placement to keep the crucial spatial context and to solve the clutter problem, we implemented three refinements that are based on this feedback:
\begin{itemize}
\item Zoom based scaling : The annotations inversely scale with the zoom level. As the user zooms in, the annotations shrink and fade a little, becoming semi transparent to reveal the points and structures underneath it.
\item Toggle visibility : A main toggle for 'Show cluster annotation”was added, allowing the user to hide all the annotations and see a clean overview.
\item Annotation design : Instead of coloring the entire annotation box background with the corresponding cluster color (which can cause contrast issues), rather the bars are colored according to the cluster color to indicate the annotation belongs to which cluster, giving an additional cue in addition to the position of the annotation, when things get dense.\\  

\end{itemize}  


\textbf{Challenge 3 Misleading cluster shapes}  

\textbf{Problem:} Early prototypes used cluster hulls, a solid colored polygon that connects the outermost points of the cluster.  

\textbf{Feedback :} Prof. Heinzl noted that these hulls were misleading, since they imply a uniform density and a sharp boundary which does not exist in the real data.

\textbf{Solution (Density view) :} We replaced the static hulls with a "Show densities" toggle. When it is activated, it renders a 2D Kernel Density Estimate (KDE)  contour map for each of the clusters. This provides a more accurate representation of cluster shape and their distribution, clearly indicating where data is dense and where it fades out.
\\  

\textbf{Challenge 4 Rigid vs fluid selection }  

\textbf{Problem:} The initial annotation only worked for pre-coimputed clusters, e.g. from a KMeans algorithm. This did not allow the user to investigate any arbitrary group of points. For example a few interesting outliers, a region where clusters overlap or some unusual protrusion from the cluster.  

 
\textbf{Feedback :} This limitation was identified through discussions with Bosch industry personnel, Thesis supervisor meetings and peer discussions. Which highlighted the need for a more fluid, custom, interactive and user-driven exploration.  

\textbf{Solution (Brushing and selection) :} We implemented a Selection mode (Shift) brushing feature, which allows the users to click and drag on any group of points in the scatterplot. The system can then instantly generate a temporary box around the selection to indicate bounds of user selection and a temporary cluster annotation is generated in the center of this new user selected box, complete with its own feature deviation bart chart. This supports for users to rapidly test their hypotheses and makes the tool a truly exploratory one as compared to a descriptive tool.
\\  

\textbf{Challenge 5 Iterative selection of Metrics}

\textbf{Problem :} Initial prototypes of DatasetWiz used the Calinski-Harabasz (CH) index as a measure for quantifying the cluster separation. However, expert consultation and iterative testing across diverse datasets and feedback showed that the CH index produced raw values ranging from $10^1$ to $10^5$. This high degree of scale sensitivity made normalization across different datasets impossible, and often resulting in skewed radar polygons that favoured datasets with higher sample sizes rather than better structural quality.

\textbf{Solution :} The CH index was remove to prevent these misleading performance rankings . While the Davies Bouldin and S-Dbw are also unbounded (0 to infinity), empirical analysis showed that those two metrics have a natural practical range, typically withing the [0,3] range for standard dataset distributions. This allowed us to implement a fixed cap normalization strategy, ensuring that the metrics still look balanced perceptually in the BarDar component. To fill the gap that was left after removing the CH index, we integrated Trustworthiness, a bounded range [0,1] metric, which is used to validate dimensionality reduction fidelity.
\\  

\textbf{Challenge 6 Scatterplot axes removal}  

\textbf{Problem:} The scatterplots had axes labellings that were not common between two dimensionality reduction methods, this added unnecessary confusion when trying to make sense of the two scatterplots. Axis labeling are somewhat helpful when it comes to DR methods like PCA, but in other non-linear methods like t-SNE, they are effectively meaningless since the scale and numerical values on the X and Y axes do not correspond to any physical unit or original feature value.

\textbf{Solution :} We intentionally removed the numerical labels and gridlines from the scatterplot axes. This design choice was based on theoretical understanding that the absolute coordinate values in non linear DR are uninterpretable. Including these axes can lead to an introduction of trust gap, where users over interpret the meaning of X and Y positions. By removing the labels, we shift the users towards the topological structure of the clusters, which are the primary information preserved by the DR algorithms.

\section{Chapter Summary} 
This chapter presented the new visualization method DimCompare, a novel, dual view interactive system that provides qualitative exploration and comparative analysis of the dimensionality reduction techniques. The key contributions are
\begin{itemize}
\item \textbf{Dual view Juxtaposition :} Enables direct, side by side visual comparison of two different dimensionality reduction projections, ex t-SNE vs PCA, allowing the users to identify the structural differences and algorithmic impact.
\item \textbf{High dimensional Cluster Annotation :}  Introduces a novel cluster annotation that explicitly encodes high dimensional feature deviations (i.e. the "why") directly onto the 2D scatterplot clusters. This fills the gap between the low dimensional projection and the original high dimensional feature space.
\item \textbf{Brushing and Selection :} Implements feature which allows users to manually select data points to generate a temporary cluster annotation for that specific selection, which allows for a deeper inspection and exploration.
\item \textbf{Integrated metric validation :} Provides dedicated metric bar charts directly underneath each of the 2D projections, offering immediate quantitative validation that links what the users perceive visually to concrete clustering metric scores.
\item \textbf{Fluid Exploratory Interaction :} Supports a flexible analytical workflow through toggles for densities, data points, and cluster annotations, as well as a selection mode that allows users to analyze arbitrary, user defined selections on the fly, all while keeping the UI and visualization clutter free.
\end{itemize} 
The next chapter introduces BarDar, a complementary tool that is designed for the high level quantitative comparison of these dimensionality reduction methods, using an aggregated, multi metric score.


\section{BarDar chart} 
BarDar is the second major contribution of this thesis, it is a novel composite visualization. It intends to tackle the limitations of traditional radar charts with barcharts to deliver a comprehensive assessment of clustering quality metrics across multiple dimensionality reduction techniques. The name BarDar reflects this integration, combining "Bar” (bar chart) and “Dar” (radar chart) into a unified visualization approach. 

\subsection{Motivation and design goals}
\subsubsection{Limitations of traditional radar charts}
Radar charts (sometimes also known as spider charts or star plots) have long been used to display multivariate data in two-dimensions and multidimensional comparison, particularly in the contexts where many attributes are addressed simultaneously. Despite their popularity in games and sports and its intuitive appeal, radar charts suffer from many limitations which are well documented in the literature \cite{feldman2013filled}, which makes them problematic for clustering quality assessment in high dimensional data analysis.

\textbf{Area Bias :} The enclosed area of the radar charts can be sometimes misleading as it may not accurately represent overall performance, and can be influenced by high or extreme values in the individual dimensions. Two methods that have very different result and performance profiles may include similar looking areas, or the other way around. The issue is made worse by the fact that the calculations in the circular or polar shape are also sensitive to the order in which the metrics are placed, the same metrics arranged differently will yield different areas. Human brains are not very adept at calculating minute area differences visually \cite{ClevelandMcGill1984GraphicalPerception}.

\textbf{Comparison Difficulty :} When two radar charts are observed side by side (juxtaposed), it is difficult to compare the areas or shapes, even when they are placed on top of each other (superimposed) the problem is only solved slightly, due to visual clutter and intersecting patterns, which make it difficult to see which technique is better overall. Overlapping polygons create complex visual patterns that require significant cognitive effort to interpret. If the number of compa  red techniques goes above 3, the visualization rapidly starts going into the unreadable territory.

\textbf{Perceptual Issues :} Human perception of angles and areas in polar coordinates is less accurate than in rectangular coordinates, which can give rise to misinterpretation of relative performance. Research in graphical perception has demonstrated that humans are way better at judging position along a common scale, like in bar charts,than judging at angles or areas \cite{ClevelandMcGill1984GraphicalPerception}

\textbf{Scale sensitive :} Different metrics operate on different scales, and with different scales/orientation, meaning sometimes higher is better and sometimes lower is better. This needs careful normalization that can sometimes obscure differences. Without doing the proper normalization, metrics with bigger ranges will take over and dominate the visual appearance, whether or not they are actually relevant in importance/interpretability.


\subsection{BarDar Design Innovation} 
BarDar addresses these limitations through a mixed design that combines multidimensional overview capability of radar charts and the precise comparison aspect of bar charts. This composite/integrated approach leverages the strengths of both types of visualization while addressing the weaknesses.

\textbf{Radar component: } This provides a visual overview of the technique performance across multiple metrics immediately and enables outlier identification and pattern recognition. Users can quickly see which metrics favour which of the techniques and spot unusual performances, hence making radar charts valuable in exploratory contexts.\cite{klippel2009shape} \cite{ward2015interactive}

\textbf{Barchart Component:} This gives precise, quantitative comparison of aggregated performance and allows for clear ranking and selection of optimal techniques. By explicitly calculating and visualising the aggregate score, the horizontal barchart removes the cognitive burden of comparing irregular polygonal shapes.

\textbf{Integrated Interaction :} Coordinated Interaction between both the components enables users to explore detailed metrics and overall performance ranking. The selection and the highlighting feature applies to both the views simultaneously. Axes can be reordered in the horizontal bar chart and it reflects in the radar chart order and overlapping.

This design follows Gleicher's taxonomy of comparative visualization , by making use of both, superposition (i.e. the overlaid radar polygons) and explicit encoding (i.e. the bar chart representation of aggregate scores. This addresses the challenge of multi-feature comparison in a fundamental manner.

\subsubsection{Theoretical justification for composite integrated design} 
The BarDar chart is designed as a composite integrated visualisation so as to overcome the well documented perceptual limitations of radar charts. While radar charts provide an effective visual signature for each of the algorithms, humans are very poor at accurately comparing the areas of irregular polygons.

According to the Cleveland and McGills \cite{ClevelandMcGill1984GraphicalPerception} study and hierarchy of graphical perception, humans are more accurate at judging position along a common scale (bar charts) than judging ares or angles (radar charts). By explicitly encoding the aggregate performance in an adjacent bar chart, BarDar reduces the cognitive load required to rank techniques while still preserving metrics using the radar polygons.



\subsection{Design Rationale}
While radar charts are frequently criticised in the visual literature, most notably in the study by Feldman \cite{feldman2013filled}, where they argue that the polygon area is an unreliable metric, due to it changing based on the ordering of the axes. The implementation in DatasetWiz is designed to reduce these pitfalls through the use of a composite interactive approach. Instead of using the chart as a static ranking too, it is used as a signature, while delegating the quantitative precision to an integrated bar chart. The rationale for this design is based on three considerations:

\textbf{Mitigating Occlusion using interactivity:}  A primary weakness identified in an earlier section is the visual clutter caused by superimposing polygons. To address this issue, BarDar makes use of translucent fills combined with high opacity hulls. It employs active z-order management, i.e. by dragging bars in the adjacent component, the user can dynamically bring the polygons that are of interest to them in the front of the rendering stack. In addition to this, users can also toggle polygons of DR methods on or off.

\textbf{Priortizing the accuracy of perception :} By following the hierarchy of graphical perception established by Cleveland and McGill \cite{ClevelandMcGill1984GraphicalPerception}, humans are significantly more accurate at judging positions across a common scale as compared to comparing irregular areas. Hence, BarDar avoids this area bias mentioned previously. During the iterative design process (see Challenge 2), we rejected the use of the shoelace formula for area calculation in favor of a mean normalized metric score. This explicit encoding in the bar chart provides a robust, order independent ranking that simplifies the user's cognitive task.

 
\textbf{Shape as a quality signature:} By decoupling the "ranking" task from the radar chart, the polygons serve their most effective purpose: pattern recognition. Instead of asking the user to calculate "which area is bigger," the radar component allows experts to identify the shape signature of a technique. For instance, a user can quickly spot if a dimensionality reduction method performs exceptionally well on "Silhouette Score" but poorly on "S\_dbw" simply by the "spike" or "dip" in the polygon, regardless of the total area.
 

\subsection{Design goals}
BarDar visualisation was designed with these goals in mind:
\begin{itemize}
\item \textbf{Comprehensive metric overview:} Display multiple quality metrics all at once in a way that reveals both individual metric values and overall patterns.
\item \textbf{Comparative ranking :} Enable immediate identification of which dimensionality reduction technique gives the best overall clustering quality for a given dataset.
\item \textbf{Detail on demand :} Allow access to individual metric values, while also providing an aggregate summary, supporting detailed and also overview analysis.
\item \textbf{Interactivity :} Support exploration through click and drag, reordering axes, tooltip popups etc.
\end{itemize}


\subsection{Radar Chart} 
\subsubsection{Metric selection and normalization}
The BarDar chart shows four main clustering quality metrics that provide complementary perspectives on cluster structure. These metrics were selected based on their established use in clustering literature and their ability to capture different aspects of cluster quality. 

\textbf{Silhouette Score :} It measures how well separated clusters are from each other, by comparing intra-cluster togetherness with inter cluster separation. The values range between -1, which indicates poor clustering, to +1, which indicates excellent clustering, and values above 5 generally indicate a good cluster structure.

\textbf{Davies Bouldin Index :} It evaluates the average similarity of each cluster with its most similar cluster, where lower values indicate better clustering, For visualization purposes, DBI is inverted during normalization to ensure consistent interpretation, of higher always being better. This metrics is also sensitive to cluster overlap.



\textbf{S\_dbw index :} This is a more recent validity index which tended to outperform other clustering indices on real and synthetic datasets [CITE original SDBW PAPER]. Lower values indicate better clustering, and also like DBI, this metric is inverted for display. S\_dbw is effective at detecting overlapping clusters and assessing density based cluster structures.


\textbf{Trustworthiness :} Unlike the other indices which focus solely on the 2D cluster structure, Trustworthiness evaluates the quality of the dimensionality reduction itself. It measures the extent to which the $k$-nearest neighbors of a point in the high-dimensional space are preserved in the low-dimensional embedding. The values range from 0 to 1, where higher values indicate that the local structure is accurately being maintained and that the projection has not introduced any "false neighbors" (points that appear close in 2D but are distant in high-dimensional space). This metric is essential for comparing DR techniques as it directly quantifies the reliability of the visual representation.


All the metrics are normalized onto a 0-1 scale to enable meaningful comparison on the same visual axes. 


\subsection{Visual Encoding} 
The radar chart uses several visual encoding strategies and mechanisms to maximize  clarity and interpretability 

\textbf{Axis layout :} Metrics are positioned at equal intervals of $90^\circ$ apart, around a circle, forming a squarish arrangement. Text labels indicate the metric names and small text annotations clarify whether higher or lower metric values are better.

\textbf{Polygon Render :} Each dimensionality reduction technique (PCA, Tt-SNE, UMAP, MDS) is represented by a distinct colored translucent polygon, which connects the metric values onto each axis. The vertices here represent normalized metric scores, and the resulting shape of the polygon provides visual performance profile of the technique.

\textbf{Grid lines :} Concentric circles at positions at 0.2, 0.4, 0.6, 0.8 and at 1.0, provide values references, without visual clutter. Lines are made up of dotted, thin, subtle light grey colors, which provides structure, without competing for visual attention with the colored polygons laid on top. The gridlines enable approximate reading of individual metric values directly from the radar chart, without the need for interaction.

\textbf{Color palette:} The color scheme selected ensures a clear distinction between the techniques, while also being accessible to colorblind users. The color assignments follow a consistent convention throughout the system:  

\begin{itemize}
\item \textbf{PCA :} Blue (\#3b82f6)  
\item \textbf{t-SNE :} Orange (\#f59e0b)  
\item \textbf{UMAP :} Green (\#22c55e)  
\item \textbf{MDS :} Red (\#ef4444)  
\end{itemize}

These colors were selected using a qualitative method, that maximizes the perceptual distinction and also maintains sufficient contrast for users who have colored vision deficiencies.

\textbf{Integrated Legend :} A legend is positioned to the top right of the radar chart, in a negative space, it provides information regarding technique name and the corresponding associated color of that technique. The legend also supports interaction, where hovering over an item in the legend, temporarily highlights the color and puts it on top of other polygons, allowing for better clarity and reducing visual confusion.




\textbf{Figure 6.1:} [PLACEHOLDER: Screenshot of BarDar radar chart showing overlaid polygons for PCA, t-SNE, UMAP, and MDS with normalized clustering quality metrics. Annotations highlight key features: concentric gridlines, metric axes with labels, colored polygons with semi-transparency, and interactive legend.]


\subsubsection{Multiple technique/radar overlay}
The radar chart can simultaneously display up to 4 different dimensionality reduction techniques, BarDar makes use of several carefully designed, visual strategies that maintain clarity given the potential for visual clutter with these many techniques.

\textbf{Transparency :} Polygons are translucent with opacities ranging between 0.2 and 0.3, to enable visibility of the overlapping regions/radars, while also maintaining the distinction between colors. The specific opacity value was determined through iterative testing to balance visibility of the individual polygons and the ability to notice overlaps.

\textbf{Stroke:} Each polygon has bold (2-3 pixel wide) borders or stroke emphasis, which ensures polygon visibility even through extensive overlaps.  The strokes are made up of the same colour as the polygon, but it's at a higher, full opacity. Doing this creates a visual hierarchy where the boundaries of each polygon remain defined, even if the fill region take on a complex shape.

\textbf{Interactivity :} The rendering order of multiple polygons affects which polygons appear on top and which appear on bottom.  Bardar uses z-order management to bring forward and highlight the radar chart of the technique that the user hovers over in the legend. The interested polygon is brought forward in the front of the rendering stack. The BarDar also employs interactivity by allowing users to reorder the rendering order of the radar charts of various techniques, by drag and drop reordering of the actual bars in the barchart component of the BarDar chart. These changes are also reflected in the order of the radar charts, where the charts are re-rendered according to the sequence set by the user.



\subsection{Barchart Integration}
The overlay design of the radar charts follows the  'superposition' principle from the Gleichers visualization taxonomy. It also augments it with the interaction and the explicit encoding via the Barchart component (which will be discussed now) to address the inherent limitations of purely superposition based visualization approaches.


\subsubsection{Score Aggregation :} 
During the development and the user testing, we also experimented with a polygonal area based approach for calculating which method performed best. The area of the polygon can be calculated easily using the shoelace formula but it was soon apparent that the area does not encode meaningful information, and it is documented to be a limitation of radar charts sensitive to the order of the axes.\cite{feldman2013filled} Our task is also of comparative ranking rather than shape analysis, and since the radar chart already provides a visual representation of the performance profile, we chose mean normalized metric score as our aggregation for its interpretability and robustness.

According to visualization literature \cite{Munzner2014} we should choose the most effective encoding for your task, which is to rank the dimensionality reduction techniques. Gleicher also states that explicit encoding of comparisons reduces the cognitive load. The barchart is an explicit encoding to simplify the  radar charts complexity, so it should use the most interpretable aggregation.

The techniques are automatically ranked by their aggregate performance, with the bars ordered from the highest to the lowest performance by default. The user can rearrange this ordering according to their analytical and exploration needs.


\subsubsection{BarChart Design}
The barchart part of BarDar makes use of familiar design conventions, while seamlessly integrating with the radar chart component, to create a unified experience

\textbf{Horizontal orientation :} Horizontal placement of the bars rather than vertical, allows for easy reading of technique name and performance values. Each bar extends from a common baseline axis (of score = 0.0), to the right side, with the length of the bars for each technique  proportional to the aggregated score for the datasets 2D projection on that metric. Horizontal was chosen over vertical because the technique names are easier to read and the general consensus is that horizontal barcharts are more suitable for nominal type of categorization (technique names like PCA, t-SNE etc), whereas vertical barcharts are more suitable when you have ordinal information like (1-5 years, 5-10 years, 10-15 years etc) \cite{Munzner2014}


\textbf{Consistent color coding :}  The bar colors match their corresponding radar chart poly gons, providing immediate visual connection between the components. This color mapping serves as a visual link that helps users track techniques across the two representations. Then the user sees an interesting polygon pattern in the radar chart component,  they can immediately locate the corresponding bar without having to read the labels.

\textbf{Value labels:} the precise percentage values are displayed at the end ofg each of the bars to provide some quantitative assessment, These labels just show the aggregated technique scores to 3 decimal places. (eg “0.927”). This enables precise comparison even when two bar lengths are visually similar.  The labels are positioned just outside the bar, to avoid occlusion with the colored bar itself.

\textbf{Bar Spacing :} The bars are spaced comfortably, with 40-50\% of the bar height used as the vertical padding between bars. This ensures that theres no label overlap and helps creates clear interactive targets, making it easy to click specific bars.

\textbf{Sorting option :} The bars can be sorted by performance values or how the users see fit, using the drag and drop functionality. They can simply put the technique they are interested in at top, and "discard"/disregard irrelevant techniques by putting it at the bottom.


\textbf{Figure 6.2:} [PLACEHOLDER: Screenshot showing the complete BarDar visualization with radar chart (top) and integrated bar chart (bottom). Annotations highlight: matching colors between radar polygons and bars, value labels on bars, sorting toggle, and coordinated highlighting across both components.]




\subsubsection{Layout and positioning}
The spatial arrangement of the radar and the bar chart components was carefully considered to support natural viewing an intuitive user experience.

\textbf{Horizontal layout :} The radar chart is positioned on the left side of the screen while the bar chart is positioned to the right side of  the screen. This arrangement follows the natural reading flow of left to right (in most cultures). The detailed multidimensional view is encountered first, on the left side, preceded by the aggregate simplified summary on the right. Initially the layout was supposed to be vertical with radar on top and barchart below, but we found out that users had to scroll down to have a look at the summary, which defeated the purpose of having a quick glance summary, therefore we opted for a design where both of the views are visible at once.

\textbf{Proportional sizing :} The radar chart occupied the 60-65\% of the horizontal space, whereas the barchart element occupied around 35-40\%. This sizing reflects the relative information density and the importance of the visualization elements. Since the radar chart contains more information (individual metric values, number of techniques and also the visually complex polygons), while the barchart only provides summary information in the form of simple horizontal bars which do not need a large space to reside.

\textbf{Responsive design :} The UI is set up in a way to support responsive design, meaning the margins and the paddings automatically adjust according to the web browser window width. The design looks good even on different types and sizes of displays like laptops, monitors, tablets and even mobile.




\subsubsection{Drag and Drop reordering}
A novel interaction feature enables reordering of techniques in the bar chart dynamically, aimed at supporting the exploratory comparison patterns that the users may find valuable.

\textbf{Drag handles :} Every bar has a subtle three vertical notches, which suggests the bars can be moved, upon hovering the mouse over the bars, it changes to a hand grab icon further indicating that they are draggable, following the principle of discoverable interfaces.

\textbf{Visual feedback :} When user starts a drag operation, many visual changes occur to provide clear feedback
The dragged bar detaches from its original position and follows the cursor
Bar gets a drop shadow to indicate separation by creating elevation 
Other bars animate smoothly to their new positions as the new dragged bar moves past them



\textbf{Reordering in the Radar Chart :} When the bars are reordered, the corresponding radar chart polygon layers are also reordered to bring the selected technique to the front. This ensures that when user focuses on a particular technique by manipulating its associated bar, that technique's polygon becomes fully visible on top even if it was earlier partially occluded by other polygons.

\textbf{Persistence :} The reordering persists until the user resets the page or applies a new order, allowing the users to arrange techniques in custom orders that support their specific analytical questions, e.g. grouping linear vs non linear methods together to compare performance etc.


Figure 6.3: [PLACEHOLDER: Sequential screenshots showing drag-and-drop reordering interaction: (1) user hovers over bar, showing drag handle, (2) during drag with bar elevated and insertion line visible, (3) after release with bars in new positions.]


\subsection{Metric normalization strategy}
\subsubsection{Normalization challenges}
Different clustering quality metric properties presented several normalisation challenges that need to be addressed to create effective visualisations

\textbf{Scale differences:} Clustering quality metrics operate on vastly different scales, which complicates direct visual comparison:
\begin{itemize}
    \item \textbf{Silhouette Coefficient:} $[-1, +1]$
	\item \textbf{Trustworthiness:} $[0, 1]$
    \item \textbf{Davies-Bouldin Index:} $[0, \infty)$ (typically $[0.3, 3.0]$)
    \item \textbf{S\_Dbw:} $[0, \infty)$ (typically $[0.1, 2.0]$)
\end{itemize}
Note: The Calinski-Harabasz Index was initially considered but removed due to its unbounded scale $[0, \infty)$ making normalization misleading and causing it to dominate visualizations.

Without appropriate normalization, metrics like the Calinski-Harabasz index would dominate the visualization purely due to their much larger values, independent of their actual analytical importance.

\textbf{Different optimization directions :} Some metrics are optimized by maximisation, (like Silhouette score and Calinski Harabasz index) whereas others are optimized by minimization (Davies Bouldin and S\_dbw). So displaying them on the same radar chart would be quite confusing. Should vertices farther from the centre indicate better performance or worse performance? The answer actually depends on the metric you're examining. 


\textbf{Dataset Dependency:}  Metrics can vary significantly across different datasets, which makes global normalization strategies problematic. A silhouette score of 0.4 might be excellent for a noisy dataset with many overlapping classes, but it will also be a poor score for a well separated score. 

\textbf{Cross dataset comparison :} When comparing metrics for different techniques in the same dataset, we want to normalize in such a fashion that maximises the spread of values across 0 to 1 range, however when comparing across the datasets, we also need the normalization that the metric maintains objective interpretations.


\subsubsection{Fixed-Threshold Normalization Approach}
BarDar uses a fixed-threshold normalization strategy designed to provide consistent, interpretable values across datasets while ensuring all metrics follow a ``higher is better'' convention.
\\  

\textbf{Silhouette Coefficient:} Maps the native $[-1, +1]$ range to $[0, 1]$:
\[
m_{\text{norm}} = \frac{m + 1}{2}
\]

\textbf{Davies-Bouldin Index:} Lower values indicate better clustering. Inverted and capped at threshold $c_{DB} = 3.0$:
\[
m_{\text{norm}} = 1 - \min\left(\frac{m}{c_{DB}}, 1\right)
\]

\textbf{S\_Dbw Index:} Lower values indicate better clustering. Inverted and capped at threshold $c_{S} = 2.0$:
\[
m_{\text{norm}} = 1 - \min\left(\frac{m}{c_{S}}, 1\right)
\]

\textbf{Trustworthiness:} Already in $[0, 1]$ with higher values indicating better local neighborhood preservation. No transformation needed:
\[
m_{\text{norm}} = m
\]

Fixed thresholds were chosen over adaptive min-max normalization to:
\begin{enumerate}
    \item Maintain consistent interpretation across different datasets
    \item Avoid misleading comparisons where a ``perfect'' score of 1.0 might represent mediocre absolute performance
    \item Ensure the normalization reflects established ranges from clustering literature
\end{enumerate}

The capping thresholds ($c_{DB} = 3.0$, $c_{S} = 2.0$) were selected based on typical value ranges observed in real-world datasets, ensuring that most practical values fall within the normalized range while extreme outliers are gracefully bounded.

\subsubsection{Limitations and tradeoffs of normalization}
While the normalization strategy effectively addresses many challenges, it introduces some new tradeoffs that users should be aware of.

\textbf{Loss of absolute interpretation :} Because the normalization is dataset specific, same normalised value (ex. Silhouette score of 0.7) may not indicate the same absolute silhouette score across different datasets, meaning that two radar charts from totally different datasets cannot be compared. Although that is not the purpose of this tool, we mitigate this issue by providing  both raw and normalised value using details on demand and tooltips.


\textbf{Loss of information :} Normalization treats all the metrics as equally important, by scaling them to [0,1] range. In reality, some metrics may be more reliable or relevant for certain data characteristics. Additionally they also might encode other information which is lost when we scale the metrics.



\subsection{User guidance and transparency}
To support the correct interpretation despite these normalization  complexities, we make use of several guidance features in BarDar chart.

\textbf{Raw Values access :} Tooltips show both normalized (used for visualization) and the raw metric outputs (raw values), enabling users to access absolute scores when needed

\textbf{Normalization Indicator :} A small text label indicates that the values are normalized and data specific.

\textbf{Metric interpretation guide:} Text tips also include simplified guidelines for each metric and their interpretation i.e. higher is better etc.


\subsection{Relationship to research questions}
BarDar directly addresses the research question 2 and 3 (RQ2, RQ3)

\textbf{RQ3 (Clustering quality visualization) :} BarDar provides a comprehensive solution for visualizing multiple clustering quality metrics simultaneously. The radar chart component shows individual metric values and patterns, whereas the bar chart component gives the aggregate assessment. Together they both allow for both detailed and also summary evaluation of the dimensionality reduction techniques effectiveness.

\textbf{RQ2 (Comparative analysis) :} The explicit encoding of aggregate performance through the bar chart enables clear comparative ranking of the dimensionality reduction methods. Unlike traditional approaches where the users must mentally compare irregular radar polygons, BarDar makes the comparison process explicit and quantitative.

Additionally it also supports Industrial use by providing a decision making support tool that helps users and practitioners select the appropriate dimensionality reduction method based on multiple quality criteria, rather than relying on singular metric or subjective visualization assessment.

\subsection{BarDar chart feedback and iteration}
The BarDar chart was designed to provide a high level summary to answer the question, 'which dimensionality reduction method produces the best clustering?'. Many challenges were encountered during the design and development of BarDar
\\  

\textbf{Challenge 1 : Perceptual non-accuracy of radar charts}  

\textbf{Initial Problem:} The initial idea was to use a single standard radar chart, where each of the vertex represents a different quality metric. The best dimensionality reduction method would theoretically have the largest  polygon.

\textbf{Feedback:} The design had two critical, and well documented flaws \cite{ClevelandMcGill1984GraphicalPerception}
\begin{enumerate}
    \item Area Comparison : Humans are notoriously bad at accurately comparing the areas of irregular polygons.
    \item Axis order : The perceived shape and area of the polygon can be dramatically altered simply by reordering the metrics around the circle in a different way, making the visualization arbitrary.
\end{enumerate}
 
\textbf{Solution :} To solve this issue, we augmented the radar chart, with the "quantitative performance” bar chart at the side of the radar chart. This chart explicitly encodes the aggregate performance of each of the DR method polygons into a simple, trivial, one dimensional bar, which gives the user a more clear and unambiguous summary.
 
\textbf{Challenge 2 : Area vs Mean}  

\textbf{Problem :} Originally, the bar chart was to simply show the actual area of the polygon, based on the logic that if metrics are higher, area will be higher, but it was soon apparent that it will suffer from the same problem of axis order dependency. Same metrics can be placed in different order around the circle to get different areas on the same dataset. The area doesn't really encode any meaningful information.

\textbf{Solution :} Instead of an area based approach for the bar chart, we decided instead to make use of an aggregation score instead, which calculates the normalized and scaled means of all the methods, which is a more closer representation of the dataset/reality than a simple radar area.

 \textbf{Challenge 3 : Nuanced comparison }  

\textbf{Problem:} The aggregate score in the bar chart is a simple normalized mean of all the metrics for that given dimensionality reduction method. The problem is it does not tell the user why a method is good, since it combines all the metrics into a single bar, possibly losing important discriminatory information.

\textbf{Feedback :} This problem was identified as a limitation during design discussions, which referenced the tool LineUp \cite{gratzl2013lineup} as an example of a more sophisticated/nuanced comparison method. The professor suggested exploring the use of stacked bar charts instead.

\textbf{Solution (Stacked Bar Chart):}  This feedback led to the development of an alternative visualization, i.e. A stacked bar chart instead of a normal bar chart. This view, intended for A/B testing, replaces the simple aggregate bar with a stacked bar chart. Each methods bar is composed of colored segments representing the individual contribution of each of the metric. This allows for a more knowledgeable user to see the precise tradeoffs and understand the composition of the final score, and not just the score itself.
 
\textbf{Challenge 4 : User flow}	

\textbf{Problem :} Original design showed the user the DimCompare view first, then the user went and explored the BarDar view. This selection was initially done arbitrarily, without any regard to the flow of the tool and user experience.

\textbf{Feedback and Solution (Reorder views) :} During the discussions, it was suggested by the research assistant that it made more sense to show the user summary view first, offered primarily by the BarDar chart, and the second view should be the DimCompare view, which offers more explanations and a qualitative analysis. Taking this suggestion, we changed the order of the views. First the user is greeted with a BarDar view which offers them a quick overview and the second view lets them dig deeper.


\section{Chapter Summary}
This chapter presented the BarDar, a novel composite visualisation combining radar charts with bar charts to address the challenge of comparing clustering quality across multiple dimensionality reduction techniques, the key contributions of BarDar include : 
\begin{itemize}
\item \textbf{Explicit aggregate encoding:} Transforms of a difficult perceptual task of comparing polygons into a simple one, i.e. comparing 1 dimensional bar heights.
\item \textbf{Accessible details :} The radar chart preserves the individual metric values, while the accompanying juxtaposed bar chart provides an aggregated summary.
\item \textbf{Interaction support :} The users can interact with the bar chart, by dragging and dropping the horizontal bars over one another, which also reflects the changes by reordering the radar chart rendering according to the new order set by the user.
\item \textbf{Adaptive Normalization :} Dataset specific normalization strategy that maximizes the discriminability while handling metrics with different scales and optimization directions.
\end{itemize}

The next chapter describes the overall system architecture and implementation details that enable BarDar and Dimcompare within an integrated web based platform/app.


\newpage
\chapter{System Design and Architecture}
In this chapter we discuss the technical implementation and system design of the visual analytics framework, i.e. DatasetWiz. The system is engineered to be a robust and scalable solution for interactive exploration and the comparative analysis of high dimensional data. The design is made to address the research questions and the practical challenges in feature space quality, as identified earlier during the industrial collaboration with Bosch.

The chapter first talks about the systems high level design and architecture patterns. It then provides component by component breakdown of the data pipeline, the analytics engine  and the novel visualizations, DimCompare and BarDar.

\section{Architectural style and Patterns}
The system makes use of a client-server architecture, to logically and physically separate the tasks of data processing from the interactive visualization. This architectural pattern is motivated by the severe computational demands of dimensionality reduction for high dimensional datasets. Non linear algorithms like t-SNE also scale poorly with data size increase.Performing all the heavy calculations server-side ensures that the frontend client remains lightweight and responsive, for rendering and interactive operations like zooming, panning and selection.

The system is made up of three primary layers:
\begin{enumerate}
\item \textbf{Presentation layer (Client) :} Browser based interface that is responsible for all the visualization rendering and user interface/interactions. It operates completely in the browser, and doesn't need any specialized installation.
\item \textbf{Application Layer (Server) :} Django (a python server framework library) based web server for managing HTTP requests, API calls, session states and routing. This acts as the central coordinator between the client and the data processing pipeline. Includes endpoints for data upload, analysis request and results delivery.
\item \textbf{Data processing Layer (Server) :} Python based analytical engine that handles computationally intensive tasks such as data parsing, normalisation, dimensionality reduction, clustering and metric calculation. It makes use of scientifically established  computing libraries to ensure correctness while maintaining good performance for handling industrial scale datasets.
\end{enumerate}  

The layered architecture enables the computations to be executed server side, where the performance can be optimized, while the interactive operations are client side so that they remain responsive. The architecture also supports future enhancements, such as distributed or parallelized computing for really big datasets.

\section{Technology Stack}
The implementation of DatasetWiz uses modern web technologies and well known scientific computation libraries both chosen for their performance, maturity and most importantly their open source nature.

\subsection{Frontend Technologies}
\begin{itemize}
\item \textbf{D3 js  v7.8.5 :} The is the foundation for all the visualisations. D3 was chosen over other high level plotting libraries like Plotly and matplotlib, for its granular, low level control of the DOM (Document Object Model) which is essential for creating novel visual encodings such as cluster annotations and the composite BarDar chart.

\item \textbf{Vanilla JavaScript ES 6+ :} Core application logic, state management and event handling.  It allows for minimal dependencies, while maintaining performance. Provides features for asynchronicity and API communication like promises and async/await.

\item \textbf{Tailwind CSS v3.0 :} CSS framework that is utility first, enabling rapid UI development with consistent styling and responsive design utilities. Used for styling the UI, buttons and other HTML elements.
\end{itemize}

\subsection{Backend Technologies}
\begin{itemize}
\item \textbf{Django 5.1.2:}  Python web framework providing application structure, URL routing, template rendering and Object Relational Management (ORM) for metadata storage. Django also provides security features like CSRF protection, SQL injection protection etc to ensure safe handling of user data.

\item \textbf{Numpy 2.0.2 :} Provides helper functions for fundamental array operations, linear algebra and numerical processing. Uses Vectorized operations to significantly accelerate the calculations.

\item \textbf{Pandas 2.2.3 :} Data wrangling, manipulation CSV parsing and feature statistics

\item \textbf{Scikit-learn 1.5.2 :} Implementation of PCA, t-SNE, MDS, k-Means clustering and standard metrics ( Silhouette, Davies Bouldin, Calinski Harabasz, Trustworthiness). The scikit-learn implementation makes use of the Barnes hut approximation for $\mathcal{O}(n\log n)$ complexity.

\item \textbf{UMAP-learn 0.5.7 :} Provides UMAP dimensionality reduction technique implementation, chosen for its superior performance on high dimensional data and the ability to preserve both local and global structures,

\item \textbf{S\_dbw 0.4.0 :} Uses the S\_Dbw clustering validity metric, developed by \cite{halkidi2001clustering}. We use a python pip library called s-dbw to calculate this score

\item \textbf{SQlite :} Lightweight relational database used for storing dataset metadata, analysis configurations and cached results.  
 

\end{itemize}  


\begin{table}[H]
	\centering
	
	\label{tab:tech_stack}
	\begin{tabularx}{\textwidth}{@{} l l X @{}}
	\toprule
	\textbf{Layer} & \textbf{Technology} & \textbf{Role and Primary Function} \\ \midrule
	\textbf{Frontend} & D3.js v7.8.5 & Low-level DOM manipulation for custom SVG visualizations (DimCompare, BarDar). \\
	 & JavaScript (ES6+) & Application state management, event handling, and asynchronous API communication. \\
	 & Tailwind CSS v3.0 & Utility-first styling for a responsive and consistent user interface. \\ \midrule
	\textbf{Backend} & Django 5.1.2 & Python web framework for URL routing, session management, and API endpoints. \\
	 & SQLite & Metadata storage and caching of dimensionality reduction results. \\ \midrule
	\textbf{Analytics} & Scikit-learn 1.5.2 & Core implementation of PCA, t-SNE, MDS, k-Means, and evaluation metrics. \\
	 & UMAP-learn 0.5.7 & High-performance implementation of the UMAP algorithm for structure preservation. \\
	 & Pandas \& NumPy & Data wrangling, CSV parsing, and vectorized numerical array operations. \\
	 & S\_dbw 0.4.0 & Specialized library for calculating density-based clustering validity. \\ \bottomrule
	\end{tabularx}
	\caption{Technical Implementation Stack of DatasetWiz}
\end{table}
 


\section{Data Processing Pipeline}
The data processing pipeline transforms the uploaded datasets through a series of modular processing and pre-processing steps.

\subsection{Data upload and validation}
When the user uploads a CSV file of a high dimensional dataset, Djangos file handling manages the upload. The system also provides immediate feedback by performing validation.  
\\  

% Erroneous line break removed

\begin{lstlisting}[language=Python, caption={Backend implementation of dataset model and initial processing.}]
	# Actual implementation from views.py
	dataset = Dataset(
		name=request.POST['name'],
		description=request.POST.get('description', ''),
		file=request.FILES['file']
	)
	dataset.save() # Triggers custom save method to:
	# 1. Save file, 2. Identify numeric/categorical columns, 3. Save metadata
	
	# Read and process numerical data
	df = pd.read_csv(dataset.file.path)
	numeric_df = df[dataset.columns['numeric']]
\end{lstlisting}


The system validated that sufficient data exists (it has minimum samples and features) and also identifies the numeric columns vs categorical columns.


[ADD PICTRE of main UI on frontend landing page/dataset uplaod]

\subsubsection{Preprocessing and Normalization}
Numerical features go through standardization to ensure that the dimensionality reduction algorithms treat all the features equally, regardless of their original scales. Since distance based algorithms (PCA, MDS, Kmeans) are highly sensitive to differences of scale, Features with larger numerical values would make everything disproportionate and dominate the analysis

The system applies Z-score Normalization (StandardScaler):

The system applies Z-score normalization (StandardScaler) to handle missing values and scale features  
\\  

\begin{lstlisting}[language=Python, caption={Data cleaning and Z-score standardization.}]
# Handle missing values - fill with column mean
for column in numeric_df.columns:
    column_mean = numeric_df[column].mean()
    numeric_df[column] = numeric_df[column].fillna(column_mean)

# Standardize using Z-score normalization
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_df)
\end{lstlisting}

This implements the transformation:
\begin{equation}
    x_{j}' = \frac{x_j - \mu_j}{\sigma_j}
\end{equation}

where $\mu_j$ and $\sigma_j$ are the mean and standard deviation of feature $j$, respectively.
  
Wherea where uj and o--j are the mean and standard deviation of feature j. This results in zero mean and unit variance, making features directly comparable.

\subsubsection{Dimensionality reduction module}
They system includes four key dimensionality reduction algorithms, each chosen to provide methodologically diverse projections 

\textbf{Principal Component Analysis (PCA) :} Linear technique preserving global variance, it is computationally efficient and also has a deterministic baseline

\textbf{t-SNE :} A non-linear technique revealing local neighborhood structures and clusters. Scikit- learn's implementation makes use of the Barnes-Hut approximation, resulting in a $\mathcal{O}(n \log n)$ performance.

\textbf{UMAP :} Modern and non-linear alternative which is based on the manifold theory. It balances preservation of both local cluster structure and global data topology with excellent speed.

\textbf{MDS :} Classic non-linear technique which preserves pairwise distances from high dimensional space in low-dimensional projection. It provides methodology/technique completeness but has $\mathcal{O}(n^3)$ complexity, which limits the scalability.


\subsubsection{Implementation}  

\begin{lstlisting}[language=Python, caption={Implementation of dimensionality reduction algorithms.}]
# Actual DR implementation from views.py
if method == 'pca':
    pca = PCA(n_components=2)
		reduced_data = pca.fit_transform(scaled_data)
	elif method == 'tsne':
		tsne = TSNE(n_components=2, random_state=42)
		reduced_data = tsne.fit_transform(scaled_data)
	elif method == 'umap':
		umap_reducer = UMAP(n_components=2, random_state=42)
		reduced_data = umap_reducer.fit_transform(scaled_data)
	elif method == 'mds':
		mds = MDS(n_components=2, random_state=42, n_init=4, 
				  dissimilarity='euclidean')
		reduced_data = mds.fit_transform(scaled_data)
	\end{lstlisting}
 
For the parameters, we use recommendations from the literature. PCA and MDS are deterministic; t-SNE uses the default $perplexity$=30. For UMAP, we use the default $n\_neighbors=15$ and set $min\_dist=0.1$.

\section{Clustering and metrics}
\subsection{High-Dimensional Clustering Strategy}
To ensure that the comparison of two DR techniques is fair, the clustering is performed once in the original high dimensional space, not in the 2D projections. Doing this provides a consistent set of cluster labels used to color both the projections, enabling a fair comparison. If the clustering is done in the low-dimensional space, the comparison cannot  be accurate since  the clustering algorithm results might change due to the presence of dimensionality reduction artifacts, as each of the DR methods produces a distinct low dimensional 2D projection. Applying the clustering algorithm to the high dimensional data allows for a true comparison.   
\\  

  

\begin{lstlisting}[language=Python, caption={K-means clustering performed on high-dimensional data.}]
# Clustering performed on HIGH-DIMENSIONAL scaled_data
if auto_detect:
    n_clusters = find_optimal_clusters(scaled_data)
else:
    n_clusters = int(data.get('n_clusters', 5))

# Use KMeans++ initialization for better convergence
kmeans = KMeans(n_clusters=n_clusters, random_state=42, init='k-means++')
clusters = kmeans.fit_predict(scaled_data)
\end{lstlisting}

If the user selects automatic detection, the system iterates through potential values of $k$ to identify the optimal number of clusters based on the highest mean Silhouette Coefficient:  
\\

\begin{lstlisting}[language=Python, caption={Automatic cluster number detection logic.}]
def find_optimal_clusters(data, max_clusters=10, min_clusters=2):
    best_score = -1
    best_k = min_clusters
    
    for k in range(min_clusters, max_clusters + 1):
        kmeans = KMeans(n_clusters=k, random_state=42)
        labels = kmeans.fit_predict(data)
        score = silhouette_score(data, labels)
        if score > best_score:
            best_score = score
            best_k = k
    return best_k
\end{lstlisting}

The k-means implementation, used as the clustering algorithm, uses the scikit-learns optimised version with k-means++ initialization, which differs from standard k-means in the sense that the initial points are chosen based on the furthest distance rather than choosing the initial points randomly, which can cause two initial starting points to be next to each other.  Users can either specify the value of $k$ or use the automatic detection which in turn is based on silhouette analysis to find the optimal number of clusters.
 


\subsection{Metric calculation and normalization}
While clustering is performed in the high dimensional space, the metrics, however, are calculated on the 2D reduced data to assess how well each of the projections represents the underlying cluster structure.  
\\

\begin{lstlisting}[language=Python, caption={Calculating clustering and projection metrics on 2D data.}]
	# Metrics calculated on 2D REDUCED data to assess projection quality
	silhouette = silhouette_score(reduced_data, clusters)
	davies = davies_bouldin_score(reduced_data, clusters)
	sdbw = calculate_sdbw(reduced_data, clusters)
	
	# Trustworthiness requires both high-D and low-D data
	trust = trustworthiness(scaled_data, reduced_data, 
							n_neighbors=min(15, len(reduced_data) - 1))
	\end{lstlisting}
 
This helps to evaluate how well the 2D projection separates the high-dimensional clusters.

\textbf{Silhouette coefficient :} Measures cluster cohesion versus separation, $s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}$ where $a(i)$ is mean intra-cluster distance and $b(i)$ is mean nearest-cluster distance. Range: [-1, 1], where higher value is better.

\textbf{Trustworthiness:} Cluster reliability or distances in high dim vs low dim.  Higher values indicate better reliability.


\textbf{Davies Bouldin index :}  Average similarity between clusters. Lower values are better.

\textbf{S\_Dbw :}  Combines the scatter and density. Lower values are better.
 
To enable meaningful visual comparison within the BarDar component, all metrics are normalized to a consistent $[0, 1]$ scale where higher values always indicate superior performance:  
\\

\begin{lstlisting}[language=Python, caption={Normalization of metrics for visual consistency in BarDar.}]
metrics = {
    'silhouette': float((silhouette + 1) / 2.0), # Map [-1,1] to [0,1]
    'trustworthiness': float(trust), # Already in [0,1]
    'davies_normalized': float(1 - min(davies / 3.0, 1.0)),# Inverted
    'sdbw_normalized': float(1 - min(sdbw / 2.0, 1.0)),# Inverted
  }
\end{lstlisting}  
  

\section{Frontend visualization implementation}
The frontend visualization renders the analytical results as interactive graphics using the d3.js library, while managing the user interactions and also maintaining the consistency across the views.

\subsubsection{DimCompare Scatterplot rendering}
\begin{lstlisting}[language=JavaScript, caption={Responsive initialization and zoom behavior in DimCompare.}]
	class DimCompare {
		constructor(container, controlsPrefix) {
			this.container = d3.select(container);
			const bbox = this.container.node().getBoundingClientRect();
			this.width = Math.max(bbox.width, 800);
			this.height = Math.max(bbox.height, 600);
			
			// Initialize linear scales for 2D projection
			this.xScale = d3.scaleLinear()
				.range([this.margin.left, this.width - this.margin.right]);
			this.yScale = d3.scaleLinear()
				.range([this.height - this.margin.bottom, this.margin.top]);
			
			// Configure zoom behavior (0.5x to 20x)
			this.zoom = d3.zoom()
				.scaleExtent([0.5, 20])
				.on('zoom', (event) => {
					this.g.attr('transform', event.transform);
					this.updateScaledElements(event.transform);
				});
		}
	}
	\end{lstlisting}

Synchronization between the scatterplots ensures coordinated exploration, through the use of linked hover events.

\subsubsection{Cluster glyph rendering and placement}
Cluster annotations are rendered as floating SVG groups positioned at the centroid of the cluster:
$$\text{centroid} = \left(\frac{1}{n}\sum_{i=1}^{n} x_i, \frac{1}{n}\sum_{i=1}^{n} y_i\right)$$


Each of the glyph displays feature deviations from the global mean. The server calculates these deviations on the original high dimensional data.  
\\

\begin{lstlisting}[language=Python, caption={Backend calculation of cluster-specific feature deviations.}]
	# Feature deviation calculation from views.py
	for cluster in range(n_clusters):
		cluster_mask = clusters == cluster
		cluster_data = numeric_df[cluster_mask]
		other_data = numeric_df[~cluster_mask]
		
		differences = {}
		for feature in selected_features:
			cluster_mean = float(cluster_data[feature].mean())
			other_mean = float(other_data[feature].mean())
			
			if other_mean != 0:
				diff_percent = ((cluster_mean - other_mean) / abs(other_mean)) * 100
			else:
				diff_percent = 0 if cluster_mean == 0 else 100
			differences[feature] = float(diff_percent)
	\end{lstlisting}


The frontend renders bar charts in the cluster annotations with logarithmic scaling, to handle percentage differences that may span across orders of magnitude (1\% to 1000\%) : 
\\
\begin{lstlisting}[language=JavaScript, caption={Logarithmic scaling for cluster glyph bar lengths.}]
	// Logarithmic bar scaling to handle wide dynamic ranges
	const logValue = Math.log10(Math.abs(value) + 1);
	const maxLogValue = Math.log10(2001); // Cap visualization at 2000%
	const barLength = Math.min((logValue / maxLogValue) * maxBarLength, maxBarLength);
	\end{lstlisting}

 
\subsection{BarDar Chart Implementation}

The BarDar visualization utilizes D3's radial line generators to construct the performance profiles of each dimensionality reduction technique.  
\\

\begin{lstlisting}[language=JavaScript, caption={Radar chart polygon rendering logic.}]
	drawRadarChart() {
		const angleSlice = (Math.PI * 2) / metricNames.length;
		const rScale = d3.scaleLinear().domain([0, 1]).range([0, maxRadius]);
		
		['pca', 'tsne', 'umap', 'mds'].forEach(method => {
			const points = metricNames.map((metric, i) => {
				const angle = angleSlice * i - Math.PI / 2;
				return {
					x: Math.cos(angle) * rScale(normalizedValue),
					y: Math.sin(angle) * rScale(normalizedValue)
				};
			});
			
			const lineGenerator = d3.line()
				.x(d => d.x)
				.y(d => d.y)
				.curve(d3.curveLinearClosed);
			
			this.radarSvg.append('path')
				.datum(points)
				.attr('d', lineGenerator)
				.attr('fill', this.colors[method])
				.attr('fill-opacity', 0.2);
		});
	}
	\end{lstlisting}

The bar chart encodes the aggregate score (mean of normalized metrics), providing a simple 1D comparison with a drag-and drop based reordering mechanism.



\section{Data Flow}
The end to end data flow of the tool looks like:

\begin{enumerate}
\item \textbf{Upload :} User uploads the CSV file via browser to Django backend
\item Preprocessing : Data processing layer parses the CSV , handles missing values if present, and applies the z-score normalization. 
\item \textbf{Clustering :} Normalized high  dimensional data is then clustered using the k-Means, generating the consistent cluster labels.
\item \textbf{Dimensionality Reduction :} Normalized data is passed to selected DR algorithms, generating a different 2D projection/coordinates.
\item \textbf{Metric calculation :} 2D reduced data and cluster labels are passed to the metrics calculation module 
\item \textbf{Aggregation :} The backend bundles the results into a JSON response
\item \textbf{Rendering :} D3.js creates scatterplots using different coordinates but using the same labels in each of the projection for coloring.
\end{enumerate}

This flow ensures users can visually compare how different DR techniques represent the exact same high dimensional cluster structure.

\subsection{Technical Specifications (table of libraryversions)}

Table 5.1: Technical Specifications
\begin{table}[h]
\centering
\label{tab:tech-stack}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Component} & \textbf{Technology} & \textbf{Version} & \textbf{Purpose} \\ \midrule
Backend Framework & Django & 5.1.2 & Application Layer, API routing, security \\
Data Processing & NumPy, Pandas & 2.0.2, 2.2.3 & Numerical computing, CSV parsing \\
DR \& Clustering & Scikit-learn & 1.5.2 & PCA, t-SNE, MDS, k-Means, metrics \\
Optimized DR & UMAP-learn & 0.5.7 & High-performance UMAP implementation \\
Custom Metrics & S\_Dbw & Custom & Cluster validity assessment \\
Frontend Rendering & D3.js & v7.8.5 & Custom visualizations (DimCompare, BarDar) \\
Frontend Logic & JavaScript & ES6+ & UI interactivity, state management \\
Frontend Styling & Tailwind CSS & v3.x & Utility-first, responsive UI design \\
Database & SQLite & - & Metadata storage \\ \bottomrule
\end{tabular}
\caption{System Implementation Stack}

\end{table}



\section{Design Trade-offs}
The final system architecture shows the intentional design tradeoffs made:
\begin{itemize}
\item \textbf{Processing (Server vs Network) :} Server side computation handles the large datasets efficiently but introduces some network latency. The benefit of robust heavy computation outweighs the network latency costs for typical high dimensional dataset sizes.
\item \textbf{Code (Vanilla JS vs frameworks) :} Using vanilla JavaScript instead of frameworks like React, nextjs or Vue.js minimizes the dependencies and gives the DOM control directly to D3.js. The tradeoff here is that we have to perform state management manually.
\item \textbf{Usability vs Original representation (BarDar metric inversions) :} Metrics like Davies Bouldin (where lower is better)  are inverted  so that "taller is always better”. This sacrifices the academic representation for user friendly interpretation.
\item \textbf{Accuracy (True data vs pretty pictures/aesthetics):} High-dimensional clustering may produce less “clean” 2D views, but this choice was deliberately made to favor rigorous evaluation over visually pleasing, but potentially misleading visualizations.
\end{itemize}

 



\section{Chapter Summary} 
This chapter details the complete system architecture and the technical implemetnaion of DatasetWiz. The key design decisions include: 
\begin{itemize}
\item \textbf{Client server architecture}: Pytho/Django backend for computation, Javascript/D3.js  frontend for responsive viualization.
\item \textbf{High dimensional data pipeline}: Clustering in te original space ensures that the projection comparison is unbiased.
\item \textbf{Technology decisions}: D3.js for low level control, scikit-learn and UMAP-learn for reliable DR implementations.
\item \textbf{Interactive Data Flow}: Asynchronous, API-driven communication enables features like the glyphs on demand etc.
\item \textbf{Design Trade-Offs}: Balancing the performance, usability, user intuitiveness
\end{itemize}
The implementation addresses the research questions through intentional and purpose built visualization components. DimCompare for RQ1 (feature interpretation), and RQ2 (side by side comparison), BarDar for RQ3 (multi-metric visualisation) and interactive features for RQ4 (pattern identification)

The next chapter presents the usage scenarios and the evaluation, demonstrating the systems effectiveness in real world scenarios using real world datasets.


\newpage

\chapter{Discussion and Evaluation}

\chapter{Conclusion}
\section{Summary}
\section{Future Work}
\section{Limitations}
\section{Conclusion}




 

\phantomsection
\addcontentsline{toc}{chapter}{\bibname}

% References Style
\iflanguage{english}
{\bibliographystyle{unsrt}} % english style - numbers citations in order of appearance
{\bibliographystyle{unsrtnat}}	% german style - numbers citations in order of appearance

\pagebreak
\bibliography{references}



%%TODO: If you don't need an Appendix, delete the next chapter before the \pagebreak command and also delete your appendix references.
%% ==============================
%\chapter{Appendix}
%\label{ch:Appendix}
%% ==============================

\pagebreak
\appendix

\iflanguage{english}
{\addchap{Appendix}}	% english style
{\addchap{Anhang}}	% german style


\section{First Appendix}   %TODO: Rename your Appendix as needed
\label{AppendixA}   
\setcounter{figure}{0}

\dots


\pagebreak


%% ==================
%DON'T: German is set as default for the declaration, don't change it
\chapter*{Eidesstattliche Erklärung}
\label{ch:Declaration}
%% ==================

Ich versichere hiermit wahrheitsgemäß, die Arbeit selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt, die wörtlich oder inhaltlich übernommenen Stellen als solche kenntlich gemacht und die Satzung der Universität Passau zur Sicherung guter wissenschaftlicher Praxis in der jeweils gültigen Fassung beachtet zu haben. Die Arbeit ist weder von mir noch von einer anderen Person an der Universität Passau oder an einer anderen Hochschule zur Erlangung eines akademischen Grades bereits eingereicht worden.

\vspace*{1cm}
\hspace*{4cm} Passau, den \submissiontime \hspace*{0.5cm}\hrulefill \\
\hspace*{10.5cm} \myname \\
\vspace*{1cm}

Ich versichere hiermit wahrheitsgemäß, dass
\begin{enumerate}
\item[\text{\ding{113}}] die Arbeit ohne Zuhilfenahme von ChatGPT oder anderen generativen KI-Werkzeugen erstellt wurde, \underline{oder}
\item[\text{\ding{113}}] ich in der nachfolgenden Tabelle vollständig dokumentiert habe, wie solche Systeme bei der Entwicklung der Arbeit verwendet wurden.
\end{enumerate}

\vspace*{1cm}
\hspace*{4cm} Passau, den \submissiontime \hspace*{0.5cm}\hrulefill \\
\hspace*{10.5cm} \myname \\

\pagenumbering{gobble}

\newpage

%% Remove the tabel if no AI tools were used
%% ==================
\begin{longtable}{p{1.3cm}lp{1.3cm}p{5cm}p{4cm}}
\caption*{Generative KI-Werkzeuge, die in der Arbeit verwendet wurden.} \label{tab:example} \\
\toprule
Kapitel & KI-Tool & Version & Prompt & Erklärung/Kommentar \\
\midrule
\endfirsthead
\multicolumn{5}{c}%
{Generative KI-Werkzeuge, die in der Arbeit verwendet wurden.} \\
\toprule
Kapitel & KI-Tool & Version & Prompt & Erklärung/Kommentar \\
\midrule
\endhead
\bottomrule
\endfoot
\bottomrule
\endlastfoot
% Insert your table here. You can also fill it out in English.
1.2 & ChatGPT & 3.5 & Schreibe einen Absatz über den Digital Markets Act. & Der generierte Output wurde in folgender Weise angepasst ...   \\
2.3 & ChatGPT & 4.0 & ... & ... \\
3.1 & ChatGPT & 3.5 & ... & ... \\
\end{longtable}
%% ==================


\end{document}

