

%%This file is used for your bibliography. 
%%You have to create entries for articles, books, inproceedings etc. 
%%which you want to cite with an At sign as follows.
%%You can create them manually (recommended) or use online sites to generate them (Important: Don't forget to check the entries!).
%%Replace the following examples with your references.


@article{zebari2020comprehensive,
  title   = {A Comprehensive Review of Dimensionality Reduction Techniques for Feature Selection and Feature Extraction},
  author  = {Zebari, Rizgar R. and Abdulazeez, Adnan M. and Zeebaree, Diyar Q. and Zebari, Dilovan A. and Saeed, Jwan N.},
  journal = {Journal of Applied Science and Technology Trends},
  volume  = {1},
  number  = {1},
  pages   = {56--70},
  year    = {2020},
  doi     = {10.38094/jastt1224},
  url     = {https://jastt.org/index.php/jasttpath/article/view/24}
}


@article{jia2022feature,
  title   = {Feature Dimensionality Reduction: A Review},
  author  = {Jia, Weikuan and Sun, Meili and Lian, Jian and Hou, Sujuan},
  journal = {Complex \& Intelligent Systems},
  volume  = {8},
  pages   = {2663--2693},
  year    = {2022},
  doi     = {10.1007/s40747-021-00637-x},
  url     = {https://doi.org/10.1007/s40747-021-00637-x}
}

@article{ANOWAR2021,
title = {Conceptual and empirical comparison of dimensionality reduction algorithms (PCA, KPCA, LDA, MDS, SVD, LLE, ISOMAP, LE, ICA, t-SNE)},
journal = {Computer Science Review},
volume = {40},
pages = {100378},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100378},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000186},
author = {Farzana Anowar and Samira Sadaoui and Bassant Selim},
keywords = {Dimension reduction, Optimal set of features, Data quality, High-dimensional datasets, Correlation metrics, Classification accuracy, Run-time},
abstract = {Feature Extraction Algorithms (FEAs) aim to address the curse of dimensionality that makes machine learning algorithms incompetent. Our study conceptually and empirically explores the most representative FEAs. First, we review the theoretical background of many FEAs from different categories (linear vs. nonlinear, supervised vs. unsupervised, random projection-based vs. manifold-based), present their algorithms, and conduct a conceptual comparison of these methods. Secondly, for three challenging binary and multi-class datasets, we determine the optimal sets of new features and assess the quality of the various transformed feature spaces in terms of statistical significance and power analysis, and the FEA efficacy in terms of classification accuracy and speed.}
}






@book{bellman1957dynamic,
  title     = {Dynamic Programming},
  author    = {Bellman, Richard Ernest},
  year      = {1957},
  publisher = {Princeton University Press},
  address   = {Princeton, NJ}
}
 

@article{peng2023interpreting,
  title   = {Interpreting the Curse of Dimensionality from Distance Concentration and Manifold Effect},
  author  = {Peng, Dehua and Gui, Zhipeng and Wu, Huayi},
  journal = {arXiv preprint arXiv:2401.00422},
  year    = {2023},
  url     = {https://arxiv.org/abs/2401.00422}
}



@Book{Munzner2014,
  author       = {Munzner, Tamara},
  publisher    = {Taylor \& Francis Ltd.},
  title        = {Visualization Analysis and Design},
  year         = {2014},
  address      = {Boca Raton, Florida},
  isbn         = {9781498759717},
  month        = dec,
  series       = {A K Peters Visualization Series},
  date         = {2014-12-01},
  doi          = {10.1201/b17511},
  ean          = {9781498759717},
  keywords     = {Information visualization},
  language     = {eng},
  pagetotal    = {428},
  url          = {https://www.ebook.de/de/product/25483296/tamara_munzner_visualization_analysis_and_design.html},
}

@article{sorzano2014survey,
  title   = {A survey of dimensionality reduction techniques},
  author  = {Sorzano, Carlos Oscar S. and Vargas, Juan and Montano, Alberto P.},
  journal = {arXiv preprint arXiv:1403.2877},
  year    = {2014},
  url     = {https://arxiv.org/abs/1403.2877}
}

@article{wani2025comprehensive,
  title   = {Comprehensive review of dimensionality reduction algorithms},
  author  = {Wani, A. A. and others},
  journal = {PMC Journal},
  year    = {2025},
  url     = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12453773/}
}


@article{jolliffe2016pca,
  title   = {Principal component analysis: a review and recent developments},
  author  = {Jolliffe, I. T.},
  journal = {Philosophical Transactions of the Royal Society A},
  volume  = {374},
  number  = {2065},
  year    = {2016},
  doi     = {10.1098/rsta.2015.0202},
  url     = {https://pubmed.ncbi.nlm.nih.gov/26953178/}
}


@article{boileau2020scPCA,
  title   = {Exploring high-dimensional biological data with sparse contrastive principal component analysis},
  author  = {Boileau, Philippe and Hejazi, Nima S and Dudoit, Sandrine},
  journal = {Bioinformatics},
  volume  = {36},
  number  = {11},
  pages   = {3422--3430},
  year    = {2020},
  doi     = {10.1093/bioinformatics/btaa176},
  url     = {https://doi.org/10.1093/bioinformatics/btaa176}
}


@article{SalihHasanAbdulazeez2021,
  title={A Review of Principal Component Analysis Algorithm for Dimensionality Reduction},
  volume={2},
  url={https://publisher.uthm.edu.my/ojs/index.php/jscdm/article/view/8032},
  abstractNote={Big databases are increasingly widespread and are therefore hard to understand, in exploratory biomedicine science, big data in health research is highly exciting because data-based analyses can travel quicker than hypothesis-based research. Principal Component Analysis (PCA) is a method to reduce the dimensionality of certain datasets. Improves interpretability but without losing much information. It achieves this by creating new covariates that are not related to each other. Finding those new variables, or what we call the main components, will reduce the eigenvalue /eigenvectors solution problem. (PCA) can be said to be an adaptive data analysis technology because technology variables are developed to adapt to different data types and structures. This review will start by introducing the basic ideas of (PCA), describe some concepts related to (PCA), and discussing. What it can do, and reviewed fifteen articles of (PCA) that have been introduced and published in the last three years.},
  number={1},
  journal={Journal of Soft Computing and Data Mining},
  author={Salih Hasan, Basna Mohammed and Abdulazeez, Adnan Mohsin},
  year={2021},
  month={Apr.},
  pages={20--30}
}
@article{scholkopf1998nonlinear,
  title   = {Nonlinear Component Analysis as a Kernel Eigenvalue Problem},
  author  = {Scholkopf, Bernhard and Smola, Alexander J. and Muller, Klaus-Robert},
  journal = {Neural Computation},
  volume  = {10},
  number  = {5},
  pages   = {1299--1319},
  year    = {1998},
  doi     = {10.1162/089976698300017467},
  url     = {https://doi.org/10.1162/089976698300017467}
}


@article{vandermaaten2008tsne,
  title   = {Visualizing Data Using t-SNE},
  author  = {van der Maaten, Laurens J.P. and Hinton, Geoffrey E.},
  journal = {Journal of Machine Learning Research},
  volume  = {9},
  pages   = {2579--2605},
  year    = {2008},
  url     = {http://www.jmlr.org/papers/v9/vandermaaten08a.html}
}

@article{kobak2019art,
  title   = {The art of using t-SNE for single-cell transcriptomics},
  author  = {Kobak, Dmitry and Berens, Philipp},
  journal = {Nature Communications},
  volume  = {10},
  number  = {1},
  pages   = {5416},
  year    = {2019},
  doi     = {10.1038/s41467-019-13056-x}
}
@misc{wattenberg2016how,
  title   = {How to Use t-SNE Effectively},
  author  = {Wattenberg, Martin and Vi√©gas, Fernanda and Johnson, Ian},
  year    = {2016},
  howpublished = {Distill},
  note    = {Online article},
  url     = {https://distill.pub/2016/misread-tsne/}
}
@article{vandermaaten2013barnes,
  title   = {Barnes-Hut t-SNE},
  author  = {van der Maaten, Laurens},
  journal = {arXiv preprint},
  year    = {2013},
  url     = {https://arxiv.org/abs/1301.3342}
}

@book{Borg2005MDS,
  author    = {Ingwer Borg and Patrick J. F. Groenen},
  title     = {Modern Multidimensional Scaling: Theory and Applications},
  publisher = {Springer},
  address   = {New York, NY},
  year      = {2005},
  edition   = {2nd},
  doi       = {10.1007/0-387-28981-X}
}
@book{KruskalWish1978,
  author    = {Joseph B. Kruskal and Myron Wish},
  title     = {Multidimensional Scaling},
  publisher = {SAGE Publications},
  series    = {Quantitative Applications in the Social Sciences},
  year      = {1978}
}

@article{Venna2010,
  author  = {Jarkko Venna and Jaakko Peltonen and Kristian Nybo and Helena Aidos and Samuel Kaski},
  title   = {Information retrieval perspective to nonlinear dimensionality reduction for data visualization},
  journal = {Journal of Machine Learning Research},
  volume  = {11},
  number  = {3},
  pages   = {451--490},
  year    = {2010}
}

@article{McInnes2018UMAP,
  author    = {Leland McInnes and John Healy and James Melville},
  title     = {UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction},
  journal   = {arXiv preprint arXiv:1802.03426},
  year      = {2018},
  url       = {https://arxiv.org/abs/1802.03426}
}

@article{Ghojogh2021UMAPSurvey,
  author    = {Benyamin Ghojogh and Ali Ghodsi and Fakhri Karray and Mark Crowley},
  title     = {Uniform Manifold Approximation and Projection (UMAP) and its Variants: Tutorial and Survey},
  journal   = {arXiv preprint arXiv:2109.02508},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.02508}
}


@article{Tenenbaum2000Isomap,
  author    = {Joshua B. Tenenbaum and Vin de Silva and John C. Langford},
  title     = {A Global Geometric Framework for Nonlinear Dimensionality Reduction},
  journal   = {Science},
  volume    = {290},
  number    = {5500},
  pages     = {2319--2323},
  year      = {2000},
  doi       = {10.1126/science.290.5500.2319}
}

@article{Hinton2006Autoencoders,
  author    = {Geoffrey E. Hinton and Ruslan R. Salakhutdinov},
  title     = {Reducing the Dimensionality of Data with Neural Networks},
  journal   = {Science},
  volume    = {313},
  number    = {5786},
  pages     = {504--507},
  year      = {2006},
  doi       = {10.1126/science.1127647}
}

@article{Espadoto,
  author    = {Mateus Espadoto and Rafael M. Martins and Andreas Kerren and Nina S. T. Hirata and Alexandru C. Telea},
  title     = {Towards a Quantitative Survey of Dimension Reduction Techniques},
  journal   = {IEEE Transactions on Visualization and Computer Graphics},
  volume    = {27},
  number    = {3},
  pages     = {2153--2173},
  year      = {2019},
  doi       = {10.1109/TVCG.2019.2934285}
}






%----------------------END OF REFERENCES----------------------


 
 
 